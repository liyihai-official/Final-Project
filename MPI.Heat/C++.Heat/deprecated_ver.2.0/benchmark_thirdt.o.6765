Loading openmpi/4.1.6-gcc-13.1.0-kzjsbji
  Loading requirement: libpciaccess/0.17-gcc-13.1.0-g4icj3q
    libiconv/1.17-gcc-13.1.0-bykwprp xz/5.4.1-gcc-13.1.0-5qb2eie
    zlib-ng/2.1.4-gcc-13.1.0-qxlbegi libxml2/2.10.3-gcc-13.1.0-inqvmk5
    ncurses/6.4-gcc-13.1.0-arr7rai hwloc/2.9.1-gcc-13.1.0-zxa33mv
    numactl/2.0.14-gcc-13.1.0-m6zhyg6 bzip2/1.0.8-gcc-13.1.0-5plj3q6
    pigz/2.7-gcc-13.1.0-omhoyt5 zstd/1.5.5-gcc-13.1.0-pf3erz3
    tar/1.34-gcc-13.1.0-w2nsvhh gettext/0.22.3-gcc-13.1.0-kqwm26r
    openssl/3.1.3-gcc-13.1.0-hj5wiwv krb5/1.20.1-gcc-13.1.0-s4cz6yy
    libedit/3.1-20210216-gcc-13.1.0-pg3u5oh libxcrypt/4.4.35-gcc-13.1.0-r653nhb
    openssh/9.5p1-gcc-13.1.0-muthxaq libevent/2.1.12-gcc-13.1.0-hjnm2t7
    pmix/5.0.1-gcc-13.1.0-vjulkd2

Strong Scaling OMP

Running with 64 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 32 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 16 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 8 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 4 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 2 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 1 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Weak Scaling OMP

Running with 1 process Grid size 128*128
rm -f main 
mpic++  -DMAX_N_X=128 -DMAX_N_Y=128 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 2 process Grid size 128*256
rm -f main 
mpic++  -DMAX_N_X=128 -DMAX_N_Y=256 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 4 process Grid size 256*256
rm -f main 
mpic++  -DMAX_N_X=256 -DMAX_N_Y=256 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 8 process Grid size 256*512
rm -f main 
mpic++  -DMAX_N_X=256 -DMAX_N_Y=512 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 16 process Grid size 512*512
rm -f main 
mpic++  -DMAX_N_X=512 -DMAX_N_Y=512 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 32 process Grid size 512*1024
rm -f main 
mpic++  -DMAX_N_X=512 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 64 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2



###############################################################################
TCHPC Cluster: callan
Job 6765 (OMP) for User 'liy35' in Account 'callan_liy35'
Finished at: Sat May 25 09:51:33 IST 2024

Job efficiency estimates:
=========================

Job ID: 6765
Cluster: callan
User/Group: liy35/liy35
State: COMPLETED (exit code 0)
Nodes: 4
Cores per node: 64
CPU Utilized: 43-07:05:58
CPU Efficiency: 18.69% of 231-15:36:32 core-walltime
Job Wall-clock time: 21:43:02
Memory Utilized: 1.00 GB
Memory Efficiency: 0.10% of 1000.00 GB

Job completion status:
======================

JobID           JobName AllocCPUS NTasks NNodes     MaxRSS    MaxRSSNode  MaxDiskRead MaxDiskWrite    Elapsed      State ExitCode 
------------ ---------- --------- ------ ------ ---------- ------------- ------------ ------------ ---------- ---------- -------- 
6765                OMP       256             4                                                      21:43:02  COMPLETED      0:0 
6765.batch        batch        64      1      1   1048876K    callan-n03     2018.61M       48.57M   21:43:02  COMPLETED      0:0 
6765.extern      extern       256      4      4          0    callan-n10        0.00M        0.00M   21:43:02  COMPLETED      0:0 


Job details:
============

JobId=6765 JobName=OMP
   UserId=liy35(60743) GroupId=liy35(60747) MCS_label=N/A
   Priority=247619 Nice=0 Account=callan_liy35 QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=21:43:02 TimeLimit=1-00:00:00 TimeMin=N/A
   SubmitTime=2024-05-24T12:08:16 EligibleTime=2024-05-24T12:08:16
   AccrueTime=2024-05-24T12:08:16
   StartTime=2024-05-24T12:08:31 EndTime=2024-05-25T09:51:33 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-05-24T12:08:31 Scheduler=Main
   Partition=compute AllocNode:Sid=localhost:3820074
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=callan-n[03,08,10-11]
   BatchHost=callan-n03
   NumNodes=4 NumCPUs=256 NumTasks=128 CPUs/Task=2 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=256,mem=1000G,node=4,billing=256
   AllocTRES=cpu=256,mem=1000G,node=4,billing=256
   Socks/Node=* NtasksPerN:B:S:C=32:0:*:* CoreSpec=*
   JOB_GRES=(null)
     Nodes=callan-n[03,08,10-11] CPU_IDs=0-63 Mem=256000 GRES=
   MinCPUsNode=64 MinMemoryCPU=4000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_third.sh
   WorkDir=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat
   StdErr=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_third.e.6765
   StdIn=/dev/null
   StdOut=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_thirdt.o.6765
   Power=
   MailUser=liy35@tcd.ie MailType=INVALID_DEPEND,BEGIN,END,FAIL,REQUEUE,STAGE_OUT
   


SLURM Bank Statement:
=====================

User           Usage |        Account     Usage | Account Limit Available (CPU hrs)
---------- --------- + -------------- --------- + ------------- ---------
liy35         15,790 |   CALLAN_LIY35    15,790 |       300,000   284,210


Acknowledgements:
=================

Note that usage of TCHPC Resources *must* be acknowledged in all publications.

Please see this page for details relevant to this cluster:

http://www.tchpc.tcd.ie/resources/acknowledgementpolicy

################################################################################
