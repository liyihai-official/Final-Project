Weak Scaling with OMP

Running with 1 process Grid size 128*128
rm -f main 
mpic++  -DMAX_N_X=128 -DMAX_N_Y=128 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 2 process Grid size 128*256
rm -f main 
mpic++  -DMAX_N_X=128 -DMAX_N_Y=256 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 4 process Grid size 256*256
rm -f main 
mpic++  -DMAX_N_X=256 -DMAX_N_Y=256 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 8 process Grid size 256*512
rm -f main 
mpic++  -DMAX_N_X=256 -DMAX_N_Y=512 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 16 process Grid size 512*512
rm -f main 
mpic++  -DMAX_N_X=512 -DMAX_N_Y=512 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 32 process Grid size 512*1024
rm -f main 
mpic++  -DMAX_N_X=512 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2

Running with 64 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT  -DUSE_OMP -fopenmp -std=c++20   main.cc -o main -O2


Strong Scaling with OMP

Running with 1 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 2 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 4 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 8 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 16 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 32 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 64 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2



###############################################################################
TCHPC Cluster: callan
Job 6728 (scalingtest) for User 'liy35' in Account 'callan_liy35'
Finished at: Thu May 23 16:25:12 IST 2024

Job efficiency estimates:
=========================

Job ID: 6728
Cluster: callan
User/Group: liy35/liy35
State: COMPLETED (exit code 0)
Nodes: 2
Cores per node: 64
CPU Utilized: 21-22:28:09
CPU Efficiency: 31.99% of 68-13:41:20 core-walltime
Job Wall-clock time: 12:51:25
Memory Utilized: 1019.86 MB
Memory Efficiency: 0.20% of 500.00 GB

Job completion status:
======================

JobID           JobName AllocCPUS NTasks NNodes     MaxRSS    MaxRSSNode  MaxDiskRead MaxDiskWrite    Elapsed      State ExitCode 
------------ ---------- --------- ------ ------ ---------- ------------- ------------ ------------ ---------- ---------- -------- 
6728         scalingte+       128             2                                                      12:51:25  COMPLETED      0:0 
6728.batch        batch        64      1      1   1044332K    callan-n04     2017.78M       48.54M   12:51:25  COMPLETED      0:0 
6728.extern      extern       128      2      2          0    callan-n04        0.00M        0.00M   12:51:25  COMPLETED      0:0 


Job details:
============

JobId=6728 JobName=scalingtest
   UserId=liy35(60743) GroupId=liy35(60747) MCS_label=N/A
   Priority=239004 Nice=0 Account=callan_liy35 QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=12:51:25 TimeLimit=1-00:00:00 TimeMin=N/A
   SubmitTime=2024-05-22T23:46:20 EligibleTime=2024-05-22T23:46:20
   AccrueTime=2024-05-22T23:46:20
   StartTime=2024-05-23T03:33:47 EndTime=2024-05-23T16:25:12 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-05-23T03:33:47 Scheduler=Backfill
   Partition=compute AllocNode:Sid=localhost:3844913
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=callan-n[04,12]
   BatchHost=callan-n04
   NumNodes=2 NumCPUs=128 NumTasks=128 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=128,mem=500G,node=2,billing=128
   AllocTRES=cpu=128,mem=500G,node=2,billing=128
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   JOB_GRES=(null)
     Nodes=callan-n[04,12] CPU_IDs=0-63 Mem=256000 GRES=
   MinCPUsNode=1 MinMemoryCPU=4000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark.sh
   WorkDir=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat
   StdErr=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_first.e.6728
   StdIn=/dev/null
   StdOut=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_first.o.6728
   Power=
   MailUser=liy35@tcd.ie MailType=INVALID_DEPEND,BEGIN,END,FAIL,REQUEUE,STAGE_OUT
   


SLURM Bank Statement:
=====================

User           Usage |        Account     Usage | Account Limit Available (CPU hrs)
---------- --------- + -------------- --------- + ------------- ---------
liy35          9,806 |   CALLAN_LIY35     9,806 |       300,000   290,194


Acknowledgements:
=================

Note that usage of TCHPC Resources *must* be acknowledged in all publications.

Please see this page for details relevant to this cluster:

http://www.tchpc.tcd.ie/resources/acknowledgementpolicy

################################################################################
