Loading openmpi/4.1.6-gcc-13.1.0-kzjsbji
  Loading requirement: libpciaccess/0.17-gcc-13.1.0-g4icj3q
    libiconv/1.17-gcc-13.1.0-bykwprp xz/5.4.1-gcc-13.1.0-5qb2eie
    zlib-ng/2.1.4-gcc-13.1.0-qxlbegi libxml2/2.10.3-gcc-13.1.0-inqvmk5
    ncurses/6.4-gcc-13.1.0-arr7rai hwloc/2.9.1-gcc-13.1.0-zxa33mv
    numactl/2.0.14-gcc-13.1.0-m6zhyg6 bzip2/1.0.8-gcc-13.1.0-5plj3q6
    pigz/2.7-gcc-13.1.0-omhoyt5 zstd/1.5.5-gcc-13.1.0-pf3erz3
    tar/1.34-gcc-13.1.0-w2nsvhh gettext/0.22.3-gcc-13.1.0-kqwm26r
    openssl/3.1.3-gcc-13.1.0-hj5wiwv krb5/1.20.1-gcc-13.1.0-s4cz6yy
    libedit/3.1-20210216-gcc-13.1.0-pg3u5oh libxcrypt/4.4.35-gcc-13.1.0-r653nhb
    openssh/9.5p1-gcc-13.1.0-muthxaq libevent/2.1.12-gcc-13.1.0-hjnm2t7
    pmix/5.0.1-gcc-13.1.0-vjulkd2

Strong Scaling NO OMP

Running with 1 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 2 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 4 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 8 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 16 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 32 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 64 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Weak Scaling NO OMP

Running with 1 process Grid size 128*128
rm -f main 
mpic++  -DMAX_N_X=128 -DMAX_N_Y=128 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 2 process Grid size 128*256
rm -f main 
mpic++  -DMAX_N_X=128 -DMAX_N_Y=256 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 4 process Grid size 256*256
rm -f main 
mpic++  -DMAX_N_X=256 -DMAX_N_Y=256 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 8 process Grid size 256*512
rm -f main 
mpic++  -DMAX_N_X=256 -DMAX_N_Y=512 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 16 process Grid size 512*512
rm -f main 
mpic++  -DMAX_N_X=512 -DMAX_N_Y=512 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 32 process Grid size 512*1024
rm -f main 
mpic++  -DMAX_N_X=512 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2

Running with 64 process Grid size 1024*1024
rm -f main 
mpic++  -DMAX_N_X=1024 -DMAX_N_Y=1024 -DHEAT   -std=c++20   main.cc -o main -O2



###############################################################################
TCHPC Cluster: callan
Job 6747 (NO-OMP) for User 'liy35' in Account 'callan_liy35'
Finished at: Fri May 24 21:25:46 IST 2024

Job efficiency estimates:
=========================

Job ID: 6747
Cluster: callan
User/Group: liy35/liy35
State: COMPLETED (exit code 0)
Nodes: 2
Cores per node: 64
CPU Utilized: 1-01:38:16
CPU Efficiency: 4.03% of 26-11:58:56 core-walltime
Job Wall-clock time: 04:58:07
Memory Utilized: 901.29 MB
Memory Efficiency: 0.18% of 500.00 GB

Job completion status:
======================

JobID           JobName AllocCPUS NTasks NNodes     MaxRSS    MaxRSSNode  MaxDiskRead MaxDiskWrite    Elapsed      State ExitCode 
------------ ---------- --------- ------ ------ ---------- ------------- ------------ ------------ ---------- ---------- -------- 
6747             NO-OMP       128             2                                                      04:58:07  COMPLETED      0:0 
6747.batch        batch        64      1      1    922916K    callan-n07     2017.14M       48.56M   04:58:07  COMPLETED      0:0 
6747.extern      extern       128      2      2          0    callan-n07        0.00M        0.00M   04:58:07  COMPLETED      0:0 


Job details:
============

JobId=6747 JobName=NO-OMP
   UserId=liy35(60743) GroupId=liy35(60747) MCS_label=N/A
   Priority=234561 Nice=0 Account=callan_liy35 QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=04:58:07 TimeLimit=1-00:00:00 TimeMin=N/A
   SubmitTime=2024-05-23T17:39:41 EligibleTime=2024-05-23T17:39:41
   AccrueTime=2024-05-23T17:39:41
   StartTime=2024-05-24T16:27:39 EndTime=2024-05-24T21:25:46 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-05-24T16:27:39 Scheduler=Main
   Partition=compute AllocNode:Sid=localhost:1761619
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=callan-n[07,12]
   BatchHost=callan-n07
   NumNodes=2 NumCPUs=128 NumTasks=128 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=128,mem=500G,node=2,billing=128
   AllocTRES=cpu=128,mem=500G,node=2,billing=128
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   JOB_GRES=(null)
     Nodes=callan-n[07,12] CPU_IDs=0-63 Mem=256000 GRES=
   MinCPUsNode=1 MinMemoryCPU=4000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_second.sh
   WorkDir=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat
   StdErr=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_second.e.6747
   StdIn=/dev/null
   StdOut=/home/users/mschpc/2023/liy35/Final-Project/MPI.Heat/C++.Heat/benchmark_second.o.6747
   Power=
   MailUser=liy35@tcd.ie MailType=INVALID_DEPEND,BEGIN,END,FAIL,REQUEUE,STAGE_OUT
   


SLURM Bank Statement:
=====================

User           Usage |        Account     Usage | Account Limit Available (CPU hrs)
---------- --------- + -------------- --------- + ------------- ---------
liy35         12,975 |   CALLAN_LIY35    12,975 |       300,000   287,025


Acknowledgements:
=================

Note that usage of TCHPC Resources *must* be acknowledged in all publications.

Please see this page for details relevant to this cluster:

http://www.tchpc.tcd.ie/resources/acknowledgementpolicy

################################################################################
