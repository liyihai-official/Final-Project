\section{N-Dimensional Matrix Implementation}
\subsection{General Setups}
Initially, we need to determine the data types to be used and define macros for 
assertions and helper functions to ensure that the program can detect common bugs 
and report their locations. 
These features can also be disabled in the release version for performance optimization. 
Such details are defined in the \texttt{assert.hpp}, \texttt{helper.hpp}, 
and other related header files located in the subfolder.

\subsection{Template Multi-dimension Matrix Detail Design}
The FDTD method is a type of FDM. 
The main idea behind FDM is briefly outlined in Section \ref{SEC:FDM}. 
The challenge lies in implementing it in a computer system to ensure it runs both correctly and efficiently.
Also, the data types I am using for storing sizes are unit32\_t and unit64\_t while I used them 
as Dworld and Qworld respectively, also are defined as size\_type and super\_size\_type in \_\_detail namespace.

\subsubsection{Performance Balancing}
\paragraph{Template Class}
Instead of doing this by using hierarchy in C++, which will cause the memory of object becomes complicated and unpredictable,
and leading to scattered data members between base and derived class objects.
This scattering can increase the cache misses which accessing these members, as the data might not be contiguous in memory.
Also, with deep inheritance hierarchies, program has higher change to occur diamond problems, since it increases the code complexity.
In detail, the diamond problem leads the duplicate inheritance and ambiguity in the method resolution, which will drop performance down again.
Although, it is solvable by using \texttt{virtual} inheritance, but again, it increase the complexity.

In such case, I chose to use template class to design the matrix object, which implement compile-time polymorphism as opposed runtime polymorphism
provided by inheritance and virtual functions. 
With such template design, the compiler makes the decisions about which function or class instantiate is made at compile time, eliminating 
the need for \texttt{vtable}s and indirect function calls, which leads to more efficient code. 

\paragraph{Memory Management}
Rather than using the standard library's (STL) vector module, 
which can be slower due to the overhead of row pointers, 
I opted to build the Matrix object using a unique pointer 
(
  \texttt{std::unique\_ptr}
), which includes only basic features such as reset, swap, and most importantly, 
a destructor that automatically deletes pointers. 
This approach enhances the safety of memory management in our programs.
Additionally, from a safety perspective, 
given that I implemented many features within the matrix object, 
I followed standard library conventions for naming. 
This includes using the \texttt{\_\_detail} namespace within namespace \texttt{multi\_array} to 
hide objects and features that are not intended for direct use by the end user.

\subsubsection{Template Object Design of Matrix Shape}

\paragraph{Strides} 
Besides that, in the multidimensional cases, the size in each dimension is not enough for accessing 
variables, this is where we need the \texttt{strides} member variable, which stores the 
number of element the operator needs to skip in each dimension.
The \texttt{\_\_multi\_array\_shape} object is encapsulated within 
the \texttt{\_\_detail} namespace and serves as a member variable of the later template object for the multi-dimensional matrix. 
This object includes a member variable defined using the STL vector, as the shape object primarily stores the sizes for each dimension, 
which typically requires only a small amount of space. 
Additionally, this object provides member functions to access the size of a given dimension. 
\begin{algorithm}
  \caption{Stride implementation}
  \begin{algorithmic}[1]
    \STATE \texttt{dims}    \hfill \# STL vector, stores the matrix's size in each dimension.
    \STATE \texttt{strides} \hfill \# STL vector, has the same size with dims.
    \STATE n = dims.size()  \hfill \# Store the dimension of matrix.
    \STATE strides[d-1] = 1; \hfill \# Stride is 1 in the first dimension.
    \FOR{ d = n - 1; d > 0; --d }     
      \STATE strides[d-1] = strides[d] * dims[d] \hfill \# Determine stride in the latter dimension.
    \ENDFOR
    \RETURN \texttt{strides}
  \end{algorithmic}
\end{algorithm}


\paragraph{Performace Balancing}
In certain scenarios, 
we only require the shape information of a matrix without needing to access the entire matrix object. 
Accessing the shape information through well-defined operators is a more efficient way to 
handle multidimensional matrices. 
This is particularly crucial in parallel programming, where understanding 
the shape of a matrix is of critical importance. 
Sometimes, a process may need to know the shape of matrices stored on other processes. 
In such cases, using this matrix object as a local variable within functions increases the 
likelihood that the compiler will store it in a register, 
which is generally faster than using heap or stack memory.
In addition, it includes check and cast functions that allow the user to verify if the template data type \texttt{\_\_T} is signed using \texttt{constexpr}. 
The \texttt{constexpr} keyword ensures that this check occurs at compile time, and if the data type is not legal, 
the program will assert and provide a message indicating that the indexing value must be a non-negative number. 


\subsubsection{Template of Multi-dimensional Matrix Implementation}
The Matrix in this project is designed to support various data types in C++. 
Consequently, the matrix is implemented as a template class with several essential features, template 
variable \texttt{\_\_T} and \texttt{\_\_NumD}, for the value data type and number of dimension, 
also including iterators, swap functionality, fill operations, and support for the IO stream operator \texttt{<<}. 
To facilitate this, the \texttt{\_\_array\_shape} object is used to explicitly manage and access the array's shape information. 


\paragraph{Operator \texttt{()}}
The hard part of this object designed is the support template number of dimension, whereas the dimension 
is integer not less than $1$,
the operator of access element is designed by following algorithm
\begin{algorithm}
  \caption{Operator \texttt{(Ext ... exts)} of template matrices object \texttt{\_\_detail::\_\_array}}
  \begin{algorithmic}[1]
    \STATE \texttt{\_\_NumD}, \texttt{\_\_T};                   \hfill \# Template variables: dimension, data type.
    \STATE \texttt{FINAL\_PROJECT\_ASSERT\_MSE}                 \hfill \# Number of Arguments must Match the dimension.
    \STATE \texttt{index} = 0, \texttt{i} = 1                \hfill \# Initialize variables in advance.
    \STATE \texttt{indices[] = \_\_shape.check\_and\_cast(ext)}          \hfill \# The indexes number must none-negative number.
    \FOR{i < \texttt{\_\_NumD}; ++i}
      \STATE \texttt{index += indices[i] * \_\_shape.strides[i]} 
    \ENDFOR
    \STATE \texttt{FINAL\_PROJECT\_ASSERT\_MSE}           \hfill \# Boundary checking.
    \RETURN \texttt{\_\_data[index]}
  \end{algorithmic}
\end{algorithm}

\paragraph{Overload operator \texttt{<<}}
In order to print the multi-dimension array with operator \texttt{<<}, I designed a recursive helper function
to print the matrix on given dimension. Thus we could call the function on the first dimension, and it will 
recursively print all dimensions.
\begin{algorithm}
  \caption{Recursive Function to Print Multi-Dimensional Array}
  \begin{algorithmic}[1]
    \STATE \texttt{current\_dim}, \texttt{offset};               \hfill \# Parameters: current dimension, offset.
    \STATE \texttt{Dims} \texttt{\_\_Dims};                       \hfill \# Template variable: number of dimensions.
    \IF{\texttt{current\_dim == \_\_Dims - 1}}
      \STATE \texttt{os << "|"}                                   \hfill \# Start printing last dimension.
      \FOR{$i$ \texttt{ from 0 to arr.\_\_shape[current\_dim] - 1}}
        \STATE \texttt{os << std::fixed << std::setprecision(5) << std::setw(9) << arr.\_\_data[offset + i];} \hfill \# Print array elements with formatting.
      \ENDFOR
      \STATE \texttt{os << " |\textbackslash n";}                  \hfill \# End of current row in the last dimension.
    \ELSE
      \FOR{$i$ \texttt{ from 0 to arr.\_\_shape[current\_dim] - 1}}
        \STATE \texttt{next\_offset = offset;}                      \hfill \# Initialize next offset.
        \FOR{$j$ \texttt{ from current\_dim + 1 to \_\_Dims - 1}}
          \STATE \texttt{next\_offset *= arr.\_\_shape[j];}          \hfill \# Update next offset based on shape.
        \ENDFOR
        \STATE \texttt{next\_offset += i * arr.\_\_shape[current\_dim + 1];} \hfill \# Finalize next offset for recursion.
        \STATE \texttt{self(self, arr, current\_dim + 1, next\_offset);} \hfill \# Recursive call to print next dimension.
      \ENDFOR
      \STATE \texttt{os << "\textbackslash n";}                     \hfill \# Print a newline after each dimension.
    \ENDIF
  \end{algorithmic}
\end{algorithm}



\subsection{Template Multi-dimension Matrix Interface Design}
With contiguity of safety, this object of multi-dimension array is accessible to users without 
direct visit to the memory space where store values of matrix.

\subsubsection{Resource Acquisition Is Initialization (RAII)}\label{SEC:RAII}
This private object has only a member variable, a unique pointer to the template \texttt{\_\_array}, 
and other member function provide necessary features to operating on it.
Smart pointers acquire resources in their constructor and automatically release them in their destructor, 
which is the essence of RAII. 
By releasing resources in the destructor, smart pointers help prevent resource leaks.
When an exception occurs, smart pointers automatically release resources, preventing resource leaks,
thus it enhanced the safety level of using resources, reduce the potential memory leak problems.


\subsubsection{Template Multi-dimension IO for writing to/reading from file}
Initially, the multi-dimension matrix has variables shape, 
and values which given dimension and size in each dimension.
This template design end up with these variable can be stored in given data types also leads with lower 
portability.
To avoid such problems and from other point of views, I chose to store the matrices in binary format, 
rather than other type files.
There are couple benefits of doing so,
\begin{enumerate}
  \item 	Compatibility and Portability: The format of binary files is relatively 
  stable and can be easily used in different programming environments or applications. 
  Unlike \texttt{.txt} files, \texttt{.mat} files those has less compatibility across different platforms.
  \item I/O Performance: Binary files can perform block-level I/O operations directly 
  without needing to parse text formats or convert data types. 
  This usually makes reading and writing binary files much faster than \texttt{.txt} files, 
  especially when dealing with large-scale multidimensional matrix data.
  \item Support MPI IO: Binary files support the MPI IO, which provides a significant reduction in the 
  cost of communication, when storing and reading the large scale matrices. 
\end{enumerate}
However, the IO does not play a critical role in effects performance of FDTD algorithms, if and only if 
we need to store or load the data during evolving the arrays.
























































\section{Parallelization of Multi-dimensional Matrices on Cartesian Topologies}


\subsection{MPI Parallel Environment Design Scheme}\label{SEC:SUB:MPI_Topology}
\subsubsection{MPI Setups}
\paragraph{Environment}
Similar to how \texttt{malloc} in C and \texttt{new} in C++ require manual memory management, 
the MPI environment also necessitates explicit initialization and finalization. 
However, unlike the efficient implementation of smart pointers in the STL, 
Boost.MPI\cite{BOOST_MPI}
-a high-level parallelism library—
may not be the optimal choice for high-performance programs. 
Therefore, I chose to design a custom MPI environment that encapsulates the necessary features specific to this project.

The \texttt{mpi} namespace, a sub-namespace of \texttt{final\_project}, provides the \texttt{environment} class. 
This class integrates MPI initialization using the constructor, which invokes \texttt{MPI\_Init\_thread},
provides multi-threading shared memory parallelism in MPI, 
and MPI finalization through the destructor, which calls \texttt{MPI\_Finalize}.

It also offers direct access to the rank and the number of processors within the MPI communicator.
Furthermore, I have explicitly deleted the copy and move assignment operators to enhance safety. 
This design decision aligns with the RAII principle, 
ensuring that MPI environment resources are automatically managed, thereby preventing leaking and 
using-uninitialized problems.

\paragraph{Types and Assertions} 
Aligned meta-programming with polymorphism principles, 
I designed a template function to retrieve the corresponding MPI basic data types, 
leveraging the fundamental data types I defined as traits at the outset.
Moreover, I provides some MPI macros in assert file, 
these macros provide a unified interface for dealing with MPI-related errors,
ensuring that MPI errors are handling consistently, safely. 

\subsubsection{MPI Topology (\texttt{Cartesian})}
The namespace \texttt{topology} is a sub-namespace of \texttt{mpi}, 
the template Cartesian structure is the mainly used object in following 
problems.
To optimize memory usage, this object maintains only essential multi-dimension matrices' global and local shape member variables.
It also contains a MPI Communicator and MPI value data type, halo data type along with the neighbors' rank in the source and dest sites.
To ensure the MPI security, the copy and move constructors as well as assignment operators are manually removed.
Additionally, the destructor is customized for properly release halo data type and Cartesian communicator. 


\paragraph{Determine the local matrix's localtion}
Evenly distributing tasks across processes is of critical importance.
To address this, I designed an algorithm to divide an integer $N$ evenly to $n$ clients, where I could put it in use in 
many cases.
Rather than implementing a standalone function, I chose to implement a lambda function, which is a feature in C++ that do not significantly 
impact the performance.
It allows me to design a small function which is not frequently use or play a key role in performance.
\begin{algorithm}
  \caption{Lambda Function (decomposition): Split tasks evenly to $n$ processes evenly}
  \label{ALG:Determine_Start_End_Decomp}
  \begin{algorithmic}[1]
    \STATE \texttt{n, rank} \hfill \# const Integers, total number and current rank of Processor.
    \STATE \texttt{N} \hfill \# constant Integer, Problem size.
    \STATE \texttt{s, e} \hfill \# Integer, start, end indexes.
    \STATE \texttt{n\_loc = n / N} \hfill \# Divide the problem evenly.
    \STATE \texttt{remain = n \% N} \hfill \# Get the remaining tasks.
    \STATE \texttt{s = rank * n\_loc + 1} \hfill \# Calculate the start indexes.
    \IF{\texttt{rank < remain}}      
      \STATE \texttt{s += rank}      \hfill \# Give a task to process the rank is smaller than remain.
      \STATE \texttt{++n\_loc}       \hfill \# Update local number of tasks.
    \ELSE 
      \STATE \texttt{s += remain}   \hfill \# Add the remain to start index, after split remains.
    \ENDIF
    \STATE \texttt{e = s + n\_loc + 1}  \hfill \# Get the ending indexes
    \IF{\texttt{e > n} \OR \texttt{rank == N - 1}} 
      \STATE \texttt{e = n}           \hfill \# If it is the end of all.
    \ENDIF
  \end{algorithmic}
\end{algorithm}
Ideally, this function will be only called when I construct the MPI topology based multi-dimension array, where I 
need cut the global matrix's shape evenly and create local matrices with local shape.
Using local shape to create local matrices is obviously a memory-saving techniques when the problem size gets larger.
Eventually, the lambda function is only applied in constructor of template Cartesian structure, which constructed from 
an input global and a MPI topology environment. 

Moreover, the topology information is determined by \texttt{MPI\_Cart\_coords} for the coordinates and \texttt{MPI\_Cart\_shift} 
the neighbors of each process in all dimension. 
Below is a example [Fig. \ref{FIG_MPI_TOPOLOGY_24_PROCS}] of cartesian topology of $24$ processors, with no period in all dimensions.
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \draw[help lines, step=1em] (0,0) grid (-20em, -20em);
    \draw[thick, ->] (0,0) -- (-20em, 0) node[left] {$x$};
    \draw[thick, ->] (0,0) -- (0, -20em) node[below] {$y$};
    
    \def\xsize{2};
    \def\ysize{3};
    \def\zsize{4};
  
    \begin{scope}[shift={(-15em, -15em)}, x={(2em,-0.25em)},y={(0em,2em)}, , z={(-1.25em,-0.5em)}]
      % Draw the front face
      \fill[blue!20] (0,0,0) -- (0,\ysize,0) -- (\zsize, \ysize,0) -- (\zsize,0,0) -- cycle;
      % Draw the top face
      \fill[blue!40] (0,\ysize,0) -- (0,\ysize,-\xsize) -- (\zsize, \ysize, -\xsize) -- (\zsize, \ysize,0) -- cycle;
      % Draw the right face
      \fill[blue!60] (\zsize,0,0) -- (\zsize,0,-\xsize) -- (\zsize, \ysize, -\xsize) -- (\zsize, \ysize,0) -- cycle;
  
      \foreach \x in {0,1, ..., \xsize}
      {
        \draw[black] (0, \ysize, -\x/\xsize * \xsize) -- (\zsize, \ysize, -\x/\xsize * \xsize);
        \draw[black] (\zsize,0,-\x/\xsize * \xsize) -- (\zsize, \ysize,-\x/\xsize * \xsize);
      }
  
      \foreach \y in {0, 1, ..., \ysize}
      {
        \draw[black] (0,\y/\ysize * \ysize,0) -- (\zsize,\y/\ysize * \ysize,0);
        \draw[black] (\zsize,\y/\ysize * \ysize,0) -- (\zsize,\y/\ysize * \ysize,-\xsize);
      }
  
      \foreach \z in {0, 1, ..., \zsize}
      {
        \draw[black] (\z/\zsize * \zsize,\ysize,0) -- (\z/\zsize * \zsize,\ysize,-\xsize);
        \draw[black] (\z/\zsize * \zsize,0,0) -- (\z/\zsize * \zsize,\ysize,0);
      }
    \end{scope}
  \end{tikzpicture}
  \caption{An example of MPI Cartesian topology Scheme of $24$ processors.}
  \label{FIG_MPI_TOPOLOGY_24_PROCS}
\end{figure}

\paragraph{Determine MPI datatypes for communication}
In order to do MPI communications, the source and the destination of every process are necessary, also the datatype.
When working with the meta-designed multidimensional matrix, we need to utilize the function \texttt{MPI\_Type\_Create\_subarray} to 
create essential halo datatypes.
\begin{algorithm}
  \begin{algorithmic}[1]
    \STATE \texttt{array\_size}, \texttt{array\_subsize}, \texttt{array\_starts=\{0\}} \hfill \#\texttt{std::array<Integer, NumD>}, the information of matrix.
    \FOR {i = 0 : NumD}
      \STATE Split tasks in dimension \texttt{i} by calling \texttt{decomposition}
      \STATE \texttt{array\_size = }local shape
      \STATE \texttt{array\_subsize = array\_size - 2}
    \ENDFOR
    \FOR {i=0 : NumD}
      \STATE \texttt{temp = array\_subsize[i]} \hfill \# Store the number temporally.
      \STATE \texttt{array\_subsize[i] = 1}
      \STATE \texttt{MPI\_Type\_Create\_subarray
      } and \texttt{MPI\_Type\_commit()} \hfill \# Create halo in dimension \texttt{i} and commit.
      \STATE Restore temporal array sub-size.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    % Draw a grid with steps of 1 cm
    \draw[help lines, step=1em] (-10em,-10em) grid (30em,30em);
  
    % Draw axes
    \draw[dashed,->] (-10em,0) -- (30em,0) node[right] {x};
    \draw[dashed,->] (0,-10em) -- (0,30em) node[above] {y};
  
    \draw[thick,->] (5em, 28em) -- (-5em, 23em) node[pos=0, above] {$source$} node[pos=0.5, above] {$dim_0$} node[pos=1, above] {$dest$};
    \draw[thick,->] (-3em, 18em) -- (-3em, 4em) node[pos=0, above] {$source$} node[pos=0.5, left] {$dim_1$} node[pos=1, below] {$dest$};
    \draw[thick,->] (14em, 28.5em) -- (28em, 27em) node[pos=0, above] {$source$} node[pos=0.5, above] {$dim_2$} node[pos=1, above] {$dest$};
  
      % Define a local macro to draw a cube at a given position
      \def\drawcube#1#2{
        \begin{scope}[shift={#1}, x={(0.8em,-0.1em)}, y={(0em,0.8em)}, z={(0.4em,0.20em)}]
          \def\size{#2}
          % Draw the front face
          \fill[blue!20] (0,0,0) -- (0,\size,0) -- (\size, \size,0) -- (\size,0,0) -- cycle;
          % Draw the top face
          \fill[blue!40] (0,\size,0) -- (0,\size,\size) -- (\size, \size, \size) -- (\size, \size,0) -- cycle;
          % Draw the right face
          \fill[blue!60] (\size,0,0) -- (\size,0,\size) -- (\size, \size, \size) -- (\size, \size,0) -- cycle;
  
          \fill[red!30] (1,1,0) -- (1,\size-1,0) -- (\size-1, \size-1,0) -- (\size-1,1,0) -- cycle;
          \fill[red!40] (1,\size,1) -- (1,\size,\size-1) -- (\size-1, \size, \size-1) -- (\size-1, \size,1) -- cycle;
          \fill[red!45] (\size,1,1) -- (\size,1,\size-1) -- (\size, \size-1, \size-1) -- (\size, \size-1,1) -- cycle;
  
          % Draw the grid
          \foreach \x in {0,1,...,\size} {
            \draw[black] (\x,0,0) -- (\x,\size,0);
            \draw[black] (0,\x,0) -- (\size,\x,0);

            \draw[black] (\size,\x,0) -- (\size,\x,\size);
            \draw[black] (\size,0,\x) -- (\size, \size,\x);

            \draw[black] (\x,\size,0) -- (\x,\size,\size);
            \draw[black] (0,\size,\x) -- (\size, \size,\x);
          }
        \end{scope}
      }
  
      % Draw the first cube at (0,0) with size 8
      \drawcube{(0em, 0em)}{10}
      \drawcube{(18em, 0em)}{10}
      \drawcube{(0em, 15em)}{10}
      \drawcube{(18em, 15em)}{10}
        
      \node[draw, minimum width = 3em, minimum height = 1.5em, align=center] at (14em,12em) {MPI Communications};
    
      \fill[red] (14em,12em) circle (3pt);
      \fill[red] (14em,4em) circle (3pt);
      \fill[red] (14em,20em) circle (3pt);

      \fill[red] (4em,12em) circle (3pt);
      \fill[red] (22em,12em) circle (3pt);
    \end{tikzpicture}  
  \caption{A 3D MPI Communication Scheme of $8$ processors between $3$ dimension matrices.}
  \label{FIG_MPI_3D_Example_SCHEME}
\end{figure}


\subsection{Template Distributed Multi-dimension Matrix Design}\label{SEC:SUB:Distributed_Matrix_Design}
\subsubsection{Detail Object Design}
Adhering to the STL safety routines, I chose to create detail template class object, hidden from users,
named \texttt{\_\_array\_Cart<class \_\_T, \_\_size\_type \_\_NumD>}.
Here, the \texttt{\_\_T} represents the value type, \texttt{\_\_size\_type} specifies the type of number of dimensions.
Since it is internal and not exposed from users, I decided to directly use other detail objects as member variables rather than 
smart pointers.
In this context, Cartesian matrix has public member variables \texttt{\_\_array} and \texttt{topology::Cartesian},
and provides memory operations. 
But the copy, move constructors and assignment operators are removed.
This approach enhances both performance both performance and simplicity by avoiding unnecessary abstractions in the internal design 
while maintain a robust memory management.

\paragraph{Distributed operator <<}
The STL os stream operator \texttt{<<} prints the matrices of all processes in sequence which build on multidimensional matrix's. 
Thus the Unix standard \texttt{fflush} function is utilized for flushing the cache in terminal, to ensure the stdout is print immediately.

\subsubsection{User Interface Design}
The \texttt{array\_Cart<class T, size\_type NumD>} is an object exposed to users, whereas only provides limited access to member variables by smart pointer.
As the size of matrices stored, it becomes clear that memory management is critically important. 
Secondly, especially in MPI distributed matrices, exposing direct memory access by set it to public member is dangerous. 
With the profits mentioned in Section \ref{SEC:RAII}, using unique pointer brings more benefits in this scenario, 
\begin{enumerate}
  \item MPI program memory management has higher complexity level. 
  Adhering RAII routines, the resources are bind with object, including MPI objects, and will be deleted as the object destructed. 
  \item Simplifying Concurrency Control. Synchronization between processes is a critical issue. 
  By using the RAII, user could unsure the resources are locked or released automatically, preventing the risk of deadlocks and resource contention.
\end{enumerate}



\subsection{Template Gather of Cartesian Distributed Multi-dimension Matrix}
The inverse of distributing multidimensional matrices 
[Sec. \ref{SEC:SUB:Distributed_Matrix_Design}]
based on MPI Topology 
[Sec. \ref{SEC:SUB:MPI_Topology}] 
is gather all distributed matrices to root from all processes.
Unlike operating on ordinary objects, gather function is operating on template objects which makes MPI operations harder.
Rather than applying full specialization for each dimension, continuing using meta-programming skills on this brings couple promotions
\begin{enumerate}
  \item Meta-programming, significantly reduce the redundancy of code, where I found the gathering operations on each dimension are highly repeatable. 
  \item Creating MPI Datatype also can be simplified due to the benefits of polymorphism. Especially when handling the high-dimension matrices.
  \item Using template makes the function is eligible to apply on various value basic data types, \texttt{Float}, \texttt{Double} etc, and provides consistency when handling uncompatible data type.
  \item Meta-polymorphism is more likely to have better performance, the initializations of template function are completed in compile time, which means there will be no run-time type-checking or type-casting.
\end{enumerate}


\subsubsection{Implement of Gather}
The \texttt{Gather} has following prototype, with the value type of entities of matrices and number of dimension as \texttt{size\_type},
\begin{lstlisting}[
  style=customCpp,
  caption={Prototypee of \texttt{final\_project::mpi::Gather}}]
  template <typename T, size_type NumD> 
  void Gather(gather, loc, root);
\end{lstlisting}
where:
\begin{itemize}
  \item \texttt{gather} is a reference of \texttt{multi\_array::array\_base<T, NumD>}, which will collect local data and store as the global matrices.
  \item \texttt{loc}    is a constant reference of the local matrices of \texttt{const array\_Cart<T, NumD>}, which holds local values and will sending data to root process.
  \item \texttt{root}   is a constant \texttt{Integer}, stands for the rank of root process.
\end{itemize}
The main idea of this function follows the following algorithm
\begin{algorithm}
  \caption{Scheme of Gather local matrices to Root process.}
  \begin{algorithmic}[1]
    \STATE Create gather object.
    \STATE Determine the sending address and sizes on each dimension, considering the boundary.
    \STATE Send the local size, starts information to root process.
    \IF{rank != root}
      \STATE Create local sub-array MPI Datatype, send to root.
    \ELSE  
      \FOR{pid = 0 : number process}
        \IF {pid != root}
          \STATE Root creates MPI Datatype using received local size information.
          \STATE Root receives data from others.
        \ELSE 
          \STATE Move the values by recursively applying local memory copy.
        \ENDIF
      \ENDFOR
    \ENDIF
  \end{algorithmic}
\end{algorithm}

\subsubsection{Determine Local Information}
Initially, the matrix object is designed on compatibility, it can read/load data from binary \texttt{.bin} files.
Thus the distributed matrices are saving  on the root process, by iterating through all processes, calculating the sizes 
and start indices on each dimension. Receiving them using \texttt{MPI\_Recv} from other processes send by \texttt{MPI\_Send}.

Considering boundaries, given a $D$ dimensional matrices, with global size $\mathcal{N}_{glob} = \{N_{glob}^d + 2\}_{d = 0}^{D-1}$.
The local matrices has shape $\mathcal{N}_{loc} = \{N_{loc}^d+2\}_{d=0}^{D-1}$, starts from $\mathcal{S} = \{s_d\}$ and ends from $\mathcal{E} = \{e_d\}$ globally.
In the $d$ dimension, as long as the matrix starts $s_d$ equals to $1$, it aligns the global boundary from source site which means the index should step back to $0$ to 
include the boundary. 
In other side, the global boundary locates at the contiguous address in $d$ dimension, 
the matrix should send a more dice in this dimension, which means the sending size $N_{loc}^{d} + 1$.
Moreover, for the special case when there is only one process. 

Adhering the RAII design, Gather function is exposed to user, thus I have not apply \texttt{swap} memory operation between Cartesian distributed matrices and none-distributed matrix just 
for handling the corner case. 
Eventually, I solve this by introducing an additional variable called \texttt{back}, which means the Root will copy for layer of data to another matrix. 
By default, \texttt{back} is $0$, and will be set to $1$ if and only if the number of process is $1$. 

With above analysis, the scheme of find local information follows 
\begin{algorithm}
  \caption{Scheme of Finding Local Sending Information: starts, shapes, indexes.}
  \label{ALG:SCHEME:Find_LOCAL_SEND_INFO}
  \begin{algorithmic}[1]
    \STATE Make copies of local shape \texttt{N\_cpy}, starts \texttt{starts\_cpy}.
    \STATE \texttt{back = 0}                           \hfill \# For handling $1$ process case.
    \IF{number process == 1}
      \STATE \texttt{++back}
    \ENDIF
    \STATE \texttt{index = \{1, 1, ..., 1\};}  \hfill \# Default sending index of local matrices.
    \FOR{d = 0 : NumD}
      \IF{starts[d] == 1}
        \STATE \texttt{-- starts\_cpy[d], --index[d]}
        \STATE \texttt{++ N\_cpy[d]}
      \ENDIF
      \IF{ends[d] == global\_shape[d] - 2}
        \STATE \texttt{++N\_cpy[d]}
      \ENDIF
      \STATE \texttt{MPI\_Gather( ..., Root)}  \hfill \# Send \texttt{starts\_cpy}, \texttt{N\_cpy} to Root;
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\subsubsection{Creating Send/Recv MPI Datatype}\label{SEC:Creating_Send/Recv_MPI_Datatype}
Local MPI communications are low efficient operations comparing to local memory copy operation, \texttt{memcpy} was used to 
recursively copy contiguous data from local matrix to global matrix exclusively on root process.
In a matrix with undefined dimension, creating data types for communicating is harder than in a specialized matrix.
For the none-root processes, they should send their data without ghost values, thus 
the \texttt{MPI\_Type\_create\_subarray} was used to create a sub-array \texttt{MPI\_Datatype} for sending message to root process.
\begin{lstlisting}[
  style=customCpp,
  caption={Routine for creating sub-array \texttt{MPI\_Datatype} as send buffer.}, 
  label={LST:mpi_subarray}]
  MPI_Type_create_subarray( 
      dimension,                /// Dimension of this array  : NumD
      array_sizes.data(),       /// shape of local array     : local_shape
      array_subsizes.data(),    /// shape of sub-array       : N_cpy
      array_starts.data(),      /// starting coordinates     : {0, 0, ..., 0}
      MPI_ORDER_C,              /// Array storage order flag : Row major in C/C++
      value_type,               /// Old MPI Datatype         : get_mpi_type<T>()
      &sbuf_block);             /// New MPI Datatype
\end{lstlisting}
The local arrays have shape $\mathcal{N}_{loc}$, and the sub-arrays have shape $\{N\_{cpy}\}_{d=0}^{D-1}$ determined by above routines 
                                            [Alg. \ref{ALG:SCHEME:Find_LOCAL_SEND_INFO}]
respectively, the starts indexes are unified as $0$s.
On the root process, the only difference is that the global shape of array \texttt{array\_sizes} are equal to the shape of global matrix.

\subsubsection{Local Copy Recursive Function}
The \texttt{memcpy} can only copy limited data to global matrix once,
since the elements we want to gather are not aligning on contiguous memory, 
but only a small amount of them - on the $0$ dimension - are continuously located.
Dut to the dimension of matrix is polymorphic and from the efficiency concern mentioned above 
            [Sec. \ref{SEC:Creating_Send/Recv_MPI_Datatype}],
the smallest copy operation using \texttt{memcpy} was encapsulated in a lambda function for recursively call and only visible in limited scope.

\begin{algorithm}
  \caption{\texttt{copy\_recursive(size\_type)}: Copy data using \texttt{memcpy} on given dimension}
  \label{}
  \begin{algorithmic}[1]
    \STATE Given current dimension \texttt{dim}.
    \IF{\texttt{dim == Num - 1}} 
      \STATE 
          \texttt{memcpy(}                                                    \hfill \# Copy data from local to global.\\
          \texttt{\: gather.begin() + gather.get\_flat\_index(loc\_idx),}     \hfill \# Get saving index in global matrix.\\
          \texttt{\: loc.data() + loc.get\_flat\_index(loc\_idx),}            \hfill \# Get copy start address of local matrix.\\
          \texttt{\: n\_list\_cpy[dim][pid] * sizeof(T) }                     \hfill \# Number of elements.\\
          \texttt{);}
    \ELSE 
      \FOR{\texttt{i = start\_cpy[dim] : loc.ends[dim] + back}}
        \STATE \texttt{local\_indexes[dim] = 1}     
        \STATE \texttt{copy\_recursive(dim + 1)}                    \hfill \# Recursive calling, copy next dimension.
      \ENDFOR
    \ENDIF
  \end{algorithmic}
\end{algorithm}

\subsubsection{Performace} \label{SEC:GATHER_PERFORMANCE}
Gather function is a computationally expensive operation when the scale is large and runs across thousands of processes. 
With this reason, gathering results is not a optimal choice for getting results in practical.
However, the performance balancing is still important since helpful for debugging and stay a health routine of coding.
In a brief, I specific designed some performance tunning features for following reasons
\begin{enumerate}
  \item Using lambda function can slow down the performance, however, gather operation is not always the core of a 
  program since it's an IO operation for debugging or get the final results at the very end. 
  Besides, it's only available when the root process is receiving data, which is a small scope compare to the other parts.
  In all the drawback can be ignored in this case.
  \item The gather matrix is only initialized when we need gather the results.
  The reason for initializing gather object in this function, is that gather matrix is conventionally very large. 
  If it is create in advance, it will consuming large amount of memory space, which will reduce the rate of memory hit rate, 
  affect the speed of latter computing.
  \item Adhering RAII routine, I chose to make an copy of each parameters of local and global metrics. 
  While these objects are relatively short, creating them locally could help us make the original data safe, 
  and make them have higher chance stored in register or cache which a lot faster than reading/writing from/to memory.
\end{enumerate}

\subsection{Parallel MPI-IO for Distributed Matrices}

Building a gather function is indeed convenient, as it allows us to collect data from all processors, 
and just using IO features of N-Dimension matrix designed previously.
However, as discussed in earlier section \ref{SEC:GATHER_PERFORMANCE},
even with optimizations on resources usage and MPI performance, the gather function still becomes 
increasingly inefficient as the problem scale grows.
Also, the gather demands more memory space as it requires enough memory on the root processor to 
store the full-scale final solution.
Given that memory is often limited and highly expensive, it is more reasonable to use MPI-IO to implement the parallel I/O,
where MPI-IO enables all processes to write results directly to disk.
Using MPI-IO offers several significant advantages:
\begin{enumerate}
\item Significantly reduces the I/O and communication burden on the root processor, 
as the root no longer needs to gather data from all other processors before performing I/O operations.
\item Completely eliminates the overhead associated with the complexity of MPI communication, 
which previously consumed substantial resources and execution time in the gather function. 
By using all processors to perform I/O operations directly to disk, this overhead is entirely removed.
\end{enumerate}
From the other perspective, 
applying MPI-IO will not improve performance of latter FDTD methods or PINNs, 
but it is a good habit for meta-programming which allows us do debugging work conveniently on larger scale 
problems.

In order to do this, \texttt{MPI\_Win\_set\_view} will be used.
Before that, the file data type (filetype) should be set first to facilitate non-contiguous memory access.
I had not chose to use \texttt{MPI\_Type\_create\_darray}, since 
it is used to generate the data types corresponding to the distribution of an N-dimensional array 
of oldtype elements onto an N-dimensional grid of logical processes without considering ghost boundaries.

Due to I have determine the start indexes using lambda function 
                                  [Alg. \ref{ALG:Determine_Start_End_Decomp}],
the \texttt{MPI\_Type\_create\_subarray} was used for creating newtype for creating file views.
Displacement was the designed for holding size of N-dimension array, and it is read/write from/to root process.
After the file view created, all processes write data into the file associated with file view collectively.
Note that the \texttt{MPI\_File\_write\_all} and \texttt{MPI\_File\_read\_all} are collective 
operations, which means the function will return only when all processes return.
However, the side-effect of such blocking operation is limited for getting final results.














































































































% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = %
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 





\section{PDE Solver Implementation}
\subsection{General Setups}
In general, the building a template solver is no longer a performance-efficient choice, since the 
optimization of compiler more unpredictable and details across different dimensions are various, 
especially for latter MPI/SMP hybrid solver implementation.
However, for the same PDE, it is quite obvious that there are many commons among different dimensional version.
If I design fully isolated objects for each dimension is an expensive idea, 
which significantly increases the complexity, redundancy and makes the program harder to maintain the consistency.
In this scenario, I chose to follow a pattern called template method pattern, which designed a base object,
includes a framework of solving this PDE and basic features required. 
This pattern has many advantages, such as 
\begin{enumerate}
  \item Significantly decrease redundancy, complexity of program, the repeated parts are extracted into base objects. 
  \item More flexible for designing features specifically for each derived objects. Makes the programer truly focus on the features designed for each object, 
  with no need for worrying the general features of base object.
\end{enumerate}


\subsection{Initial/Boundary Condition Object Design}
\subsubsection{Initial Condition Class}
\texttt{InitialConditions} is the \texttt{namespace} where all initial condition classes located.
In the long tern, this it will include initial conditions for heat equation in different dimension space, so as other type of PDEs.
For this project, I implemented two classes, \texttt{Init\_2D} and \texttt{Init\_3D} for 2D and 3D heat equations.
Both of the classes have private member \texttt{std::function} objects, 
that store the function in mathematical form $\varphi(\vec{x})$ , such as the equation \ref{EQ:Heat2D}.
Default constructors of them are set the $\varphi(\vec{x}) = 0$, once the classes are initialized, the 
boolean value \texttt{isSetUpInit} will be set to \texttt{true}.
Since inits objects are \texttt{friend} of the PDEs objects, user can apply this objects to PDEs objects by calling \texttt{SetUpInit}.
The syntax \texttt{friend} brings several benefits
\begin{enumerate}
  \item It will enhance encapsulation, especially in this case, init objects and PDEs object don not need closely work together,
  exposing internal implementation details to each other would violate the principles of encapsulation. 
  \item Also, it helps keep the class interface clean, only
  exposing the necessary members while keeping the rest the implementation details hidden.
\end{enumerate}


\subsubsection{Boundary Condition Class}
Boundary Condition is \texttt{friend} class of PDE classes that located on cube domain, and can be a tedious and hard implementation, 
due to there are $6$ functions in 3D space and $4$ in 2D space.
Thus, I had not chose to use virtual inheritance pattern on this implementation.
Moreover, there are mainly three type of boundary conditions, Dirichlet, Von Neumman and Robin, also known as first, second and third type 
of boundary conditions.

In this project, I chose to imply first and second type conditions, which denoted with 
member variable \texttt{std::array<Bool, 2*NumD> isDirchletBC} and \texttt{std::array<Bool, 2*NumD> isNeumannBC}.
Similarly to init classes, the \texttt{SetBC} is also a function for apply this boundary condition to PDE objects and the status of setup 
is stored as a boolean \texttt{isSetUpBC}.
However, unlike the initial condition, the Von Neumman boundary conditions need to be update during evolving processes.
Thus, an additional member called \texttt{UpdateBC} is designed for handling this case.
This member function will update the boundaries which is set as Von Neumman conditions and leave the Dirichlet conditions' alone.

Implementing boundary conditions can be a tough task, since the domain of a problem commonly is commonly not regular shape such as cube or cylinder.
Especially the derivatives are required in the Von Neumman and Robin conditions.





\subsection{PDE Solver Objects Design}
For solving a PDE system, it's necessary to store the parameters of basic this PDE.
Take Heat Equation [E.q. \ref{EQ_HEAT_EQ_EG}] as an example, the domain $\Omega = \left[0,1\right]^2$, and coefficient $\lambda = 1$.
Besides that, there are extra setups needed for the FDTD methods' iteration, including 
\begin{enumerate}
  \item Determine time and space step sizes and other parameters needed from FDTD.
  \item Exchanging (Communication) between different processors.
  \begin{enumerate}
    \item Using blocking MPI communication methods, \texttt{MPI\_Sendrecv}.
    \item Using non-blocking MPI communication methods, \texttt{MPI\_Irecv}, \texttt{MPI\_Isend}.
  \end{enumerate}
  \item Updating (Evolving) function to compute the values of next status in time domain.
  \begin{enumerate}
    \item Purely MPI, no threading updating strategy.
    \item Hybrid of MPI/OpenMP, MPI process's has threading.
  \end{enumerate}
\end{enumerate}

\subsubsection{Abstract Base Class}
The default copy/move constructors, copy/move assignment operators of base object are deleted 
due to it is abstract base class and only provides a unified design.
The constructor, member variables and virtual functions are set as protected, which can be accessed by derived classes exclusively.
\paragraph{Constructor} 
Constructor takes template arguments of extents as inputs, and using extents to determine domain and grid sizes.
According to the general setups in section \ref{SEC:Specific_Form}, and CFL rules, the $D$ dimension heat equation has time step size
\begin{equation}
  \Delta t \leqslant \frac{1}{ 
    \displaystyle 
    2 \lambda 
    \sum_{i=1}^{D} \frac{1}{\Delta x_i ^2}
  }
\end{equation}
Moreover, I chose to use the modified 
                                          CFL \cite{CFL} 
shown below 
\begin{equation}
  \Delta t 
  \leqslant 
  \frac{ \min_{i=1}^{D} \{\Delta x_i\}^2}{2 D\lambda}
  \leqslant 
  \frac{1}{ 
    \displaystyle 
    2 \lambda 
    \sum_{i=1}^{D} \frac{1}{\Delta x_i ^2}
  }
\end{equation}
The weights and diagonal coefficients are 
\begin{equation}
  \begin{cases}
    \displaystyle w_i = \frac{\lambda \Delta t}{\Delta x_i^2}               \\
    \displaystyle diag_i = -2 + \frac{\Delta x_i^2}{2\lambda\Delta t}
  \end{cases}
\end{equation}

\paragraph{Pure virtual functions }
Virtual functions overcome the problems with type-field solution, which I used in earlier deprecated versions, 
it allowing me to declare functions in the base class in advance, and redefined in each derived class.
The compiler and linker will guarantee the correct correspondence between objects and the functions applied to them.
The type-field solution has several drawbacks:
\begin{enumerate}
  \item Using constructs like \texttt{switch-case} and \texttt{if-else} requires frequent modifications to the conditional expressions whenever a new type is added.
  \item The reliance on type casting increases the risk of errors, making the program more prone to runtime issues.
  \item As the number of types increases, the codebase becomes increasingly redundant and difficult to maintain.
\end{enumerate}

The \texttt{virtual} functions in base class stands for
\begin{itemize}
  \item \texttt{virtual Exchanging} functions mean that the derived class will provide specific definition of different exchange strategies.
  \item \texttt{virtual Evolving} functions shows the derived classes have $2$ different hybrid updating strategies.
\end{itemize}

\subsubsection{PDE solver class}
Derived template classes PDE solver objects are 
\texttt{Heat\_2D<typename T>} and 
\texttt{Heat\_3D<typename T>} inherit from 
\texttt{Heat\_Base<typename T, size\_type NumD>} 
with level 
\texttt{protected}. 
Each PDE solver class has integrated solver functions, and stands for two types of methods that are pure MPi parallelism 
                                      [Sec. \ref{SEC:PURE_MPI}] and hybrid parallelism.
Moreover, the hybrid parallelistic solver has two type of hybrid strategies which I will demonstrate in latter section and section
                                       \ref{SEC:HYBRID_0} and \ref{SEC:HYBRID_1}.

\paragraph{Safety}
The PDE solvers are unified interface objects with integrated IO features, solvers and functions allow to interacting with boundary conditions and initial condition objects,
same as previous sections, I chose to use unique pointer to create objects of boundary and initial conditions which adhere RAII protocol.
Also, redefining \texttt{virtual} functions of base class using \texttt{override final} phrases that means these functions are not safe for overloading if latter program want to 
override in second derived classes. 

\paragraph{Performance}
Every solver class has two objects of Cartesian distributed arrays, the updating strategy I was using is called ping-pong strategy or red-black strategy.
This requires us to update values between two objects, one of which store the current status, the other for next status.
Obviously, this is a memory-hungary strategy, which requires twice as much as the actual memory space needed for storing local array.
However, due to our parallelistic topology, the local arrays are quite smaller than the actual scale of problem, more importantly, 
it increase the efficiency of updating values in one CPU clockwise.



\subsection{Parallel Strategies}
The MPI-3.x introduced shared memory programming unlike OpenMP library, 
Independent Software Vendor (ISV) and application libraries need not to be thread-safe
No additional OpenMP overhead and problems. 
In general, the Parallel Programming Models on Hybrid Platforms have 
                                              \cite{SUPERsmith}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    % Draw a grid with steps of 1 cm
    \def\gridsize{5em};
    \draw[help lines, step=1em] (-\gridsize,-\gridsize) grid (\gridsize,\gridsize);
  
    % Draw axes
    \draw[thick,->] (-\gridsize,0) -- (\gridsize,0) node[right] {$x$};
    \draw[thick,->] (0,-\gridsize) -- (0,\gridsize) node[above] {$y$};    
  \end{tikzpicture}
  \caption{Hybrid Platforms\cite{SUPERsmith}}
  \label{FIG:Hybrid_Platforms}
\end{figure}
In this project, I chose to implement three parallel strategies
\begin{itemize}
  \item Pure MPI: only one MPI process on each core.
  \item hybrid MPI + OpenMP with non-overlapped communication/computation:
                          inter-node communication OpenMP: inside of each SMP node, 
                          MPI only outside of parallel regions of the numerical application code.
  \item funneled hybrid + Open with overlapped communication/computation: 
                          MPI communication by one or a few threads while other threads are computing.
\end{itemize}

\subsubsection{Pure Message Passing Parallel} \label{SEC:PURE_MPI}
Pure MPI parallelism is a type of strategy that do not take shared memory programming into considering.
With no worry about NUMA structure, pure MPI only needs to determine what is the efficient way for communication,
such as 
\begin{enumerate}
  \item Blocking communication, it is a type of communication with internal barrier, which is only favorable when we need to ensure the latter operations depend on this MPI communication,
  such as reduction, gather, broadcasting and scattering.
  \item Non-blocking communication, it is the other type of communication with no barrier. 
  This is advantageous in scenarios where subsequent operations do not depend on the completion of the communication, 
  allowing computation and communication to overlap, which can improve overall efficiency.
  \item Remote memory access (RMA), the need for explicit send/receive pairs, RMA operation is directly access memory of other CPUs.
  This can be very efficient, particularly on systems with hardware support like RDMA (Remote Direct Memory Access), 
  However, on systems without such hardware support, 
  RMA may not provide performance benefits and could be slower compared to traditional message-passing methods.
\end{enumerate}
Thus, this projects focused on blocking and non-blocking MPI communications, since RMA requires the hardware support to gain the benefits.
In general, Pure MPI strategy follows algorithm
\begin{algorithm}
  \caption{Pure MPI PDE solver Mechanism}
  \label{ALG:PURE_MPI}
  \begin{algorithmic}[1]
    \STATE The $i^{th}$ iteration 
    \STATE Determine current time of $i^{th}$ step.
    \STATE Exchange ghost values.
    \STATE Update, store in the next status.
    \STATE Update boundary conditions at current time.
    \STATE \texttt{MPI\_Allreduce} the local difference check if it is converge
    \IF {No Debug}
      \STATE Store the current results if in Debug mode.  \hfill \# Debug mode
      \STATE Print global difference on root process.
    \ENDIF
    \STATE Switch current / next arrays.
    \IF{glocal difference $\leqslant$ tolerance}
      \STATE converge = \texttt{true}, break the iteration.
    \ENDIF
  \end{algorithmic}
\end{algorithm}
The update function \texttt{update\_ping\_pong()} follows the values except ghost values, as the idea of FDM described in equation \ref{EQ_FDM_FDTD_EQ_Iterating}
or more specifically for a $D$ dimension heat equation
\begin{equation}
  u^{n+1}_{1,\dots,d,\dots,D} = 
      u^{n}_{1, \dots,d, \dots,D}
    + \sum_{d=1}^{D}\frac{\lambda \Delta t}{\Delta x_d^2}
  \left(
      u^{n}_{1,\dots,d+1, \dots,D} 
    - 2u^{n}_{1,\dots,d, \dots,D} 
    + u^{n}_{1,\dots,d-1, \dots,D}
  \right)
\end{equation}

\paragraph{Exchange}
Blocking communication is applied with \texttt{MPI\_Sendrecv} function which is a unified operation of 
an \texttt{MPI\_Send} to the destination site 
and an \texttt{MPI\_Recv} from source site.
The communication topology is shown in figure \ref{FIG_MPI_3D_exchange_SCHEME}, 
for each process, there are two \texttt{MPI\_Sendrecv} operations, for the destination and source sites communications.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    % Draw a grid with steps of 1 cm
    % \draw[help lines, step=1em] (-2em,-2em) grid (20em,20em);
  
    % Draw axes
    % \draw[dashed,->] (-2em,0) -- (20em,0) node[right] {x};
    % \draw[dashed,->] (0,-2em) -- (0,20em) node[above] {y};
    
      % Define a local macro to draw a cube at a given position
      \def\drawcube#1#2{
        \begin{scope}[shift={#1}, x={(0.8em,-0.1em)}, y={(0em,0.8em)}, z={(0.4em,0.20em)}]
          \def\size{#2}

          \draw[thick,->] (-2em, 3em) -- (1em, 2.5em) node[pos=0, above] {$source$} node[pos=0.5, above] {} node[pos=1, above] {};
          \draw[thick,->] (3em, 2em) -- (3em, -2em) node[pos=0, above] {} node[pos=0.5, left] {} node[pos=1, below] {$dest$};
          \draw[thick,->] (9em, 5em) -- (2em, 2em) node[pos=0, above] {$source$} node[pos=0.5, above] {} node[pos=1, above] {};

          % Draw the front face
          \fill[blue!20] (0,0,0) -- (0,\size,0) -- (\size, \size,0) -- (\size,0,0) -- cycle;
          % Draw the top face
          \fill[blue!40] (0,\size,0) -- (0,\size,\size) -- (\size, \size, \size) -- (\size, \size,0) -- cycle;
          % Draw the right face
          \fill[blue!60] (\size,0,0) -- (\size,0,\size) -- (\size, \size, \size) -- (\size, \size,0) -- cycle;
  
          \fill[red!30] (1,1,0) -- (1,\size-1,0) -- (\size-1, \size-1,0) -- (\size-1,1,0) -- cycle;
          \fill[red!40] (1,\size,1) -- (1,\size,\size-1) -- (\size-1, \size, \size-1) -- (\size-1, \size,1) -- cycle;
          \fill[red!45] (\size,1,1) -- (\size,1,\size-1) -- (\size, \size-1, \size-1) -- (\size, \size-1,1) -- cycle;
          
          \draw[thick,->] (5em, 2em) -- (8em, 1.5em) node[pos=0, above] {} node[pos=0.5, above] {} node[pos=1, above] {$dest$};
          \draw[thick,->] (3em, 7em) -- (3em, 4em) node[pos=0, above] {$source$} node[pos=0.5, left] {} node[pos=1, below] {};
          \draw[thick,->] (2em, 2em) -- (-3em, 0em) node[pos=0, above] {} node[pos=0.5, above] {} node[pos=1, above] {$dest$};

          % Draw the grid
          \foreach \x in {0,1,...,\size} 
          {
            \draw[black] (\x,0,0) -- (\x,\size,0);
            \draw[black] (0,\x,0) -- (\size,\x,0);

            \draw[black] (\size,\x,0) -- (\size,\x,\size);
            \draw[black] (\size,0,\x) -- (\size, \size,\x);

            \draw[black] (\x,\size,0) -- (\x,\size,\size);
            \draw[black] (0,\size,\x) -- (\size, \size,\x);
          }

        \end{scope}
      }
  
      % Draw the first cube at (0,0) with size 8
      \drawcube{(0em, 0em)}{5}
      \drawcube{(14em, 0em)}{5}
      \drawcube{(0em, 12em)}{5}
      \drawcube{(14em, 12em)}{5}

    \end{tikzpicture}  
  \caption{A 3D MPI Communication Scheme of $8$ processors between $3$ dimension matrices.}
  \label{FIG_MPI_3D_exchange_SCHEME}
\end{figure}


% The average time consumption of a process denoted with $t_{b}$ stands for blocking communications, 
% due to the $p^{th}$ will return when the send and recv operation complete. 
% Thus the $p^{th}$ process among $n_p$ processes, it will wait $(p - 1)t_{b}$ time to complete.
% Ideally, the execution time would close to $pt_b$, however, non-blocking communication has time complexity $O(n_p^2)$.

\subsubsection{No comm./comp overlapped Hybrid Parallel}\label{SEC:HYBRID_0}
The hybrid programming takes shared memory into consider, and the most simple idea is using OpenMP multi-threads for updating 
arrays of each MPI process.

In the meanwhile, the update function \texttt{update\_ping\_pong} has a nested loop which is suitable for using \texttt{parallel for} syntax.
Thus the update function of the first hybrid strategy is \texttt{update\_ping\_ping\_omp} with OpenMp pragma 
\texttt{omp for collapse(NumD)}.
In order to collect differences from all threads of MPI processes, the local difference of threads are collected to MPI local differences
by using pragma \texttt{omp critical}, with a \texttt{omp barrier} at the end.
The boundary condition updates and gathering results or print global difference are set to \texttt{single}.
\begin{algorithm}
  \caption{Master-only MPI+OpenMP with non-overlapped Communication/Computation}
  \label{ALG:Hybrid_0}
  \begin{algorithmic}[1]
    \FOR {iterations}
      \STATE \texttt{\#pragma omp single}
      \STATE MPI communications.
      \FOR {for grid values}
        \STATE \texttt{\#pragma omp for} updates.
      \ENDFOR
      \STATE \texttt{\#pragma omp single} Switch and other operations.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}




\paragraph{Exchange}
The exchange method of first hybrid solver is identical with pure MPI exchanger, however, only single thread will call it.
Such strategy is called Master-Only Hybrid 
                                                      \cite{Master-Only}, 
it can be using \texttt{omp single} for calling MPI communications
rather than using \texttt{omp master}.
In general the master-only hybrid has such communication scheme
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \def\drawMPI#1#2{
      \begin{scope}
        \draw[node] (0em, 0em);
      \end{scope}
    }

    % Draw a grid with steps of 1 cm
    \def\gridsize{5em};
    \draw[help lines, step=1em] (-\gridsize,-\gridsize) grid (\gridsize,\gridsize);
  
    % Draw axes
    \draw[thick,->] (-\gridsize,0) -- (\gridsize,0) node[right] {$x$};
    \draw[thick,->] (0,-\gridsize) -- (0,\gridsize) node[above] {$y$};


  \end{tikzpicture}
  \caption{Scheme: Master only hybrid strategy}
  \label{FIG:master_only_hybrid}
\end{figure}


\subsubsection{Funneled Hybrid Parallel with overlapped comm./comp.}\label{SEC:HYBRID_1}
Basic on the work in section \ref{SEC:HYBRID_0}, the second hybrid strategy is still master-only hybrid but with more optimizations on update 
function. 
Due to the inner part of grid is independent with ghost values, thus the update function can be separated the update function into two steps, 
update the bulk and update the edges.

\begin{algorithm}
  \caption{Funneled Master-only MPI+OpenMPI with overlapped Communication/Computation}
  \label{ALG:Hybrid_1}
  \begin{algorithmic}[1]
    \FOR {iterations}
      \STATE \texttt{\#pragma omp single}
      \STATE MPI communications. (\texttt{Isends/Irecvs})
      \FOR {for bulk points} 
        \STATE Update bulk points.                                  \hfill \# Update the points that do not need ghosts.
      \ENDFOR
      \STATE \texttt{MPI\_Waitall()}                                \hfill \# Wait all MPI Communications complete
      \FOR {for edge pints}
        \STATE update edge values.                                 
      \ENDFOR
      \STATE \texttt{\#pragma omp single} Switch and other operations.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
Comparing to the previous scheme \ref{ALG:Hybrid_0}, the scheme \ref{ALG:Hybrid_1} overlaps part of MPI communications and computation, 
rather than only take the advantages of shared memory computing.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \def\drawMPI#1#2{
      \begin{scope}
        \draw[node] (0em, 0em);
      \end{scope}
    }

    % Draw a grid with steps of 1 cm
    \def\gridsize{5em};
    \draw[help lines, step=1em] (-\gridsize,-\gridsize) grid (\gridsize,\gridsize);
  
    % Draw axes
    \draw[thick,->] (-\gridsize,0) -- (\gridsize,0) node[right] {$x$};
    \draw[thick,->] (0,-\gridsize) -- (0,\gridsize) node[above] {$y$};


  \end{tikzpicture}
  \caption{Scheme: Master only hybrid strategy with computation/communication overlapping}
  \label{FIG:master_only_hybrid_1}
\end{figure}
Also, due to the asynchronous communication is not guaranteed by non-blocking MPI communication,
which means that the implementation of multiple overlapping of communication / computation.
Thus, applying multiple overlapping hybrid method can be very hard to do even on simple question, and highly hardware dependent.
% Besides that, load balancing is a problem as well since the true load of each processes is hidden before we actually run it.
% Thus, it is really hard 


% \subsection{Physics Informed Neural Networks}
% \subsection{CUDA parallel}
% \subsection{Hybrid Parallel}