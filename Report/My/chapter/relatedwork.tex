\section{Related Work}
% 在工程学、物理学、生物学和化学等众多领域，为了解决各种各样的偏微分方程，学术界尝试了非常多的途径，诞生了很多数值解法并且都已经非常成熟。
% 有限差分法是一种被广泛运用的方法，通过差商来近似所需要的连续函数的导数。

To gain well quality solution of various types of PDEs is prohibitive and notoriously challenging.
The number of methods available to determine canonical PDEs is limited as well,
includes 
separation of variables, 
superposition, 
product solution methods, 
Fourier transforms, 
Laplace transforms and 
perturbation methods, 
among a few others.
Even though there methods are exclusively well-performed on constrained conditions,
such as regular shaped geometry domain, constant coefficients, well-symmetric conditions 
and many others.
These limits strongly constrained the range of applicability of numerical techniques for solving PDEs,
rendering them nearly irrelevant for solving problems practically.

General, the methods of determining numerical solutions of PDEs can be broadly classified into
two types: 
deterministic 
and stochastic. 
The mostly widely used stochastic method for solving PDEs is 
Monte Carlo Method \cite{Monte_Carlo_Method} which is a popular method in solving PDEs in higher dimension space with 
notable complexity.

\subsection{Finite Difference Method}\label{SEC:FDM}
The FDM is based on the numerical approximation method in calculus of finite differences.
The motivation is quiet straightforward which is approximating solutions by finding values satisfied PDEs on a set of 
prescribed interconnected points within the domain of it. 
% \begin{wrapfigure}{r}{0.45\textwidth}
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.5\textwidth]{figure/FIG_POSSION_2D_10_NODES.jpg}
  \begin{tikzpicture}
    \foreach \x/\y [count=\i from 1] in {
      1.6/1.4, %1
      2.3/2.4, %2
      4/0.8,  %3
      3.7/1.5, %4
      4.3/2.1, %5
      3.3/2.4, %6
      1.8/3.9, %7
      1.2/2.2, %8
      3.9/3.1, %9
      2.8/3.3, %10
      3/1, %11
      5/2, %12
      4/4, %13
      1/3, %14
      1/1} %15
      { 
        \coordinate (P\i) at (\x,\y);
      % \fill[red] (\x,\y) circle (3pt);
      % \node[above right] at (\x,\y) {P\i};
  }
    \draw[thick, fill=green!10] (P11) 
      .. controls (4,0)   and (5,1)   .. (P12)
      .. controls (4.5,4) and (5,4.5) .. (P13) 
      .. controls (3,3.5) and (1,5)   .. (P14)
      .. controls (1,2)   and (0,2)   .. (P15) 
      .. controls (2,0)   and (2,2)   .. (P11);
  
  \foreach \x in {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15} 
  {
    \fill[red] (P\x) circle (2pt);    % 填充红色圆点
    \node[above right] at (P\x) {$\x$}; % 标记点的标签
  }

  % \node[above right] at (0,0) {$O$};
  \draw[thick, ->] (0,0) -- (1,0) node[right, right] {$x$};
  \draw[thick, ->] (0,0) -- (0,1) node[left, left] {$y$};

  \draw[thick,-] (P2) -- (P1);
  \draw[thick,-] (P2) -- (P6);
  \draw[thick,-] (P2) -- (P7);
  \draw[thick,-] (P2) -- (P10);
  \draw[thick,-] (P2) -- (P8);
  \draw[thick,-] (P2) -- (P4);

  \draw[thick,-] (P6) -- (P10);
  \draw[thick,-] (P6) -- (P9);
  \draw[thick,-] (P6) -- (P5);
  \draw[thick,-] (P6) -- (P4);

  \draw[thick,-] (P10) -- (P7);
  \draw[thick,-] (P10) -- (P9);
  \draw[thick,-] (P10) -- (P13);

  \draw[thick,-] (P9) -- (P5);
  \draw[thick,-] (P9) -- (P13);
  \draw[thick,-] (P12) -- (P13);
  \draw[thick,-] (P9) -- (P12);

  \draw[thick,-] (P5) -- (P4);
  \draw[thick,-] (P5) -- (P12);
  \draw[thick,-] (P3) -- (P12);
  \draw[thick,-] (P4) -- (P12);

  \draw[thick,-] (P4) -- (P3);
  \draw[thick,-] (P1) -- (P8);
  \draw[thick,-] (P1) -- (P15);
  \draw[thick,-] (P15) -- (P8);
  \draw[thick,-] (P15) -- (P11);
  \draw[thick,-] (P15) -- (P14);
  \draw[thick,-] (P7) -- (P14);
  \draw[thick,-] (P7) -- (P13);

  \draw[thick,-] (P14) -- (P2);
  \draw[thick,-] (P8) -- (P14);

  \draw[thick,-] (P11) -- (P3);
  \draw[thick,-] (P11) -- (P1);
  \draw[thick,-] (P11) -- (P4);
  \draw[thick,-] (P11) -- (P2);


  \end{tikzpicture}
  \caption{The Schematic Representa of of a 2D Computiational Domain and Grid. The nodes are used for the FDM by solid circles. 
  Nodes $11-15$ denote boundary nodes, while nodes $1-10$ denote internal nodes.}
  \label{FIG_POSSION_2D_10_NODES}
\end{figure}
% \end{wrapfigure}
Those points are which referred as nodes, and the set of nodes 
are so called as a grid of mesh.
A notable way to approximate derivatives are using 
Taylor Series expansions.
Taking 2 dimension Possion Equation as instance, assuming the investigated value as, $\varphi$,
\begin{equation}\label{EQ_POSSION_2D}
  \frac{
    \partial ^ 2 \varphi  
  }{
    \partial x ^ 2
  } +
  \frac{
    \partial ^ 2 \varphi  
  }{
    \partial y ^ 2
  }
  =  f(x,y)
\end{equation}
The total amount of nodes is denoted with $N = 15$, which gives the numerical equation which governing 
equation \ref{EQ_POSSION_2D} 
shown in 
equation \ref{EQ_POSSION_2D_10_NODES} and nodes layout as shown in the 
figure \ref{FIG_POSSION_2D_10_NODES}
\begin{equation}\label{EQ_POSSION_2D_10_NODES}
  \frac{
    \partial ^ 2 \varphi_i
  }{
    \partial x_i ^ 2
  } +
  \frac{
    \partial ^ 2 \varphi_i
  }{
    \partial y_i ^ 2
  }
  =  f(x_i,y_i) = f_i, \:\:\:\:\: i = 1,2,\dots,15
\end{equation}
In this case, we only need to find the value of internal nodes which $i$ is ranging from $1$ to $10$.
Next is aimming to solve this linder system \ref{EQ_POSSION_2D_10_NODES}.


\subsection{Physics Informed Neural Networks}\label{SEC:PINN}
With the explosive growth of avaliable data and computing resouces, 
recent advances in machine learning and data analytics have yieled good results across science discipline, 
including Convolutional Neural Networks (CNNs) \cite{CNN}
for image recoginition, 
Generative Pre-trained Transformer (GPT) \cite{GPT}
for natual language processing and 
Physics Informed Neural Networks (PINNs) \cite{PINN}
for handling science problems with high complexity.
PINNs is a type of machine learning model makes full use of the benefits from 
Auto-differentiation (AD) \cite{AD}
which led to the emergence of a subject called 
Matrix Calculus \cite{Matrix_Calculus}.
Considering the parametrized and nonlinear PDEs of the general form [E.q. \ref{EQ_General_PDEs}] of function $u(t,x)$
\begin{equation}\label{EQ_General_PDEs}
  u_t + \mathcal{N}\left[u;\lambda\right] = 0
\end{equation}
The $\mathcal{N}[\cdot;\lambda]$ is a nonlinear operator which parametrized by $\lambda$.
This setup includes common PDEs problems like heat equation, and black-stokz equation and others.
In this case, we setup a neural network $NN[t,x;\theta]$ which has trainable weights $\theta$ and takes 
$t$ and $x$ as inputs, outputs with the predicting value $\hat{u}(t,x)$.
In the training process, the next step is calculating the necessary derivatives of $u$ with the respect to $t$ and $x$.
The value of loss function is a combination of the metrics of how well does these predictions fit the given conditions and 
fit the natural law [Fig. \ref{FIG_Schematic_View_PINN}]. 
% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.5\textwidth]{figure/FIG_Schematic_View_PINN.jpg}
%   \caption{The Schematic Representa of a structure of PINN.}
%   \label{FIG_Schematic_View_PINN}
% \end{figure}


\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
    % Define styles
    neuron/.style={circle,draw,minimum size=.2cm},
    func/.style={rectangle,draw,minimum size=.2cm},
    PDE/.style={func, fill=green!20},
    loss/.style={func, fill=blue!20},
    input/.style={neuron, fill=green!20},
    hidden/.style={neuron, fill=blue!20},
    output/.style={neuron, fill=red!20},
    diff/.style={neuron, fill=green!20},
    arrow/.style={->,>=stealth},
    scale=0.5
  ]
    
  % \steporfull<1->{
    % Input Layer
    \draw[dashed, thick] (-2,1) rectangle (9,-5); % 前两个坐标为矩形左下角的坐标，后两个坐标为矩形右上角的坐标
    \node at (-2,1) [above right] {$NN(t,x; \theta)$}; % 文本内容为"文本"，位置为方框的左上角

    \node[input] (Input0) at (0,-1) {$x$};
    \node[input] (Input) at (0,-3) {$t$};
  
    % Hidden Layer 1
    \node[hidden] (Hidden11) at (2,0) {$\sigma$};
    \node[hidden] (Hidden12) at (2,-2) {$\sigma$};
    \node[hidden] (Hidden13) at (2,-4) {$\sigma$};
  
    % Hidden Layer 2
    \node[hidden] (Hidden21) at (4,0) {$\sigma$};
    \node[hidden] (Hidden22) at (4,-2) {$\sigma$};
    \node[hidden] (Hidden23) at (4,-4) {$\sigma$};
  
    % Hidden Layer 3
    \node[hidden] (Hidden31) at (6,0) {$\sigma$};
    \node[hidden] (Hidden32) at (6,-2) {$\sigma$};
    \node[hidden] (Hidden33) at (6,-4) {$\sigma$};

    % Output Layer
    \node[output] (Output) at (8,-2) {$u$};

    % Connect neurons Input-Hidden Layer 1
    \foreach \i in {1,2,3}
        \draw[arrow] (Input) -- (Hidden1\i);
      \foreach \i in {1,2,3}
        \draw[arrow] (Input0) -- (Hidden1\i);
  
    % Connect neurons Hidden Layer 1-Hidden Layer 2
    \foreach \i in {1,2,3}
        \foreach \j in {1,2,3}
            \draw[arrow] (Hidden1\i) -- (Hidden2\j);
  
    % Connect neurons Hidden Layer 2-Hidden Layer 3
    \foreach \i in {1,2,3}
        \foreach \j in {1,2,3}
            \draw[arrow] (Hidden2\i) -- (Hidden3\j);
  
    % Connect neurons Hidden Layer 3-Output
    \foreach \i in {1,2,3}
        \draw[arrow] (Hidden3\i) -- (Output);
  % }

  % \steporfull<2->{
    \draw[dashed, thick] (7,2.5) rectangle (18,-6.4);
    \node at (15,2.5) [above right] {PDE($\lambda$)}; % 文本内容为"文本"，位置为方框的左上角
    % Partial Derivatives
    \node[diff] (D1) at (10,1) {$\frac{\partial}{\partial t}$};
    \node[diff] (D2) at (10,-2) {$\frac{\partial}{\partial x}$};
    \node[diff] (D3) at (10,-5) {$\frac{\partial^2}{\partial^2 x}$};
    \node[PDE] (PDE) at (14.5,-2) {$u_t(t,x) + \varGamma \left[u; \lambda \right]$};

    \foreach \i in {1,2,3}
        \draw[arrow] (Output) -- (D\i);
    \foreach \i in {1,2,3}
        \draw[arrow] (D\i) -- (PDE);
  % }

  % \steporfull<2->{
    \node[loss] (L) at (12,-7.3) {Loss};

    \draw[arrow] (Output) |- (L);
    \draw[arrow] (PDE) |- (L);
  % }
  \end{tikzpicture}
  \caption{The Schematic Representation of a structure of PINN, a FCN with 4 layers (3 hidden layers)}
  \label{FIG_Schematic_View_PINN}
\end{figure}

\subsection{Finite Difference Time Domain Method}
As described previously in the 
section \ref{SEC:FDM}, FDM could solve the PDEs in its original form where 
Finite Element and Finite Volume Methods gained results by solving modified form such as an integral form of the governing equation.
Though the latter methods are commonly get better results or less computational hungery, 
the FDM has many descendants, 
for instance the Finite Difference Time Domain Method (FDTD) where it still finds prolific usage are computational heat equation and 
computational electromagnetics (Maxwell's equations). 
Assuming the operator $\mathcal{N}[\cdot;\lambda]$ is set to $\nabla$ where it makes E.q. \ref{EQ_General_PDEs} become 
to heat equation \ref{EQ_HEAT_EQ_EG}.
\begin{equation}\label{EQ_HEAT_EQ_EG}
  \frac{\partial u}{\partial t} 
  - \lambda \left(
    \frac{\partial^2 u}{\partial x^2}  + \frac{\partial^2 u}{\partial y^2} 
  \right) = 0
\end{equation}
Using the key idea of FDM, assuming the step size in spatio-time space are $\Delta x$, $\Delta y$ and $\Delta t$,
we could have a series equations which have form [E.q. \ref{EQ_FDM_FDTD_EQ}].
\begin{equation}\label{EQ_FDM_FDTD_EQ}
  \frac{u^{n+1}_{i,j} - u^n_{i,j}}{\Delta t} 
  = 
  \lambda
  \left(
    \frac{
      u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1,j}
    }{\Delta x^2}
    +
    \frac{
      u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i,j+1}
    }{\Delta y^2}
  \right)
\end{equation}
when the time step size satisfies the 
Couran, Friedrichs, and Lewy condition (CFL\cite{CFL_limitation}).
We could get the strong results by iterating the equation \ref{EQ_FDM_FDTD_EQ}, or more specifically using 
equation \ref{EQ_FDM_FDTD_EQ_Iterating} to get the value $u$ of next time stamp $n+1$ on nodes $i,j$.
\begin{equation}\label{EQ_FDM_FDTD_EQ_Iterating}
  u^{n+1}_{i,j}
  = 
  u^n_{i,j} + 
    \frac{
      \lambda\Delta t
    }{\Delta x^2}\left(
      u^n_{i+1,j} - 2u^n_{i,j} + u^n_{i-1,j}
    \right)
    +
    \frac{
      \lambda\Delta t
    }{\Delta y^2}\left(
      u^n_{i,j+1} - 2u^n_{i,j} + u^n_{i,j+1}
    \right)
\end{equation}
also shown in the figure \ref{FIG:Scheme_FDTD_2D_Heat}.
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=0.8, transform shape]
    \def\h{10};
    \def\scale{0.8};

    \begin{scope}
      \myGlobalTransformation{0}{0};
      \graphLinesHorizontal;
      \graphLinesVertical;
    \end{scope}
    \graphThreeDnodes{0}{0};

    \begin{scope}
      \myGlobalTransformation{0}{\h};
      \graphLinesHorizontal;
      \graphLinesVertical;
    \end{scope}
    \graphThreeDnodes{0}{\h};

    \begin{scope}
      \myGlobalTransformation{0}{0};
      \node (orig) at (0,0) [circle, fill=black, scale=0.45] {};
      \node (x) at (4,0) {};
      \node (y) at (0,4) {};

      \node (this) at (4,4) [circle, fill=red, scale=\scale] {};

      \node (left) at (2.5,4) [circle, fill=red, scale=\scale] {};
      \node (back) at (4,2.5) [circle, fill=red, scale=\scale] {};
      \node (right) at (5.5,4) [circle, fill=red, scale=\scale] {};
      \node (front) at (4,5.5) [circle, fill=red, scale=\scale] {};
    \end{scope}

    \begin{scope}
      \myGlobalTransformation{0}{12};
      \node (Horig) at (0,0) {};
    \end{scope}

    \begin{scope}
      \myGlobalTransformation{0}{\h};
      \node (other) at (4,4) [circle, fill=red, scale=\scale] {};
    \end{scope}

    \draw[very thick,->] (this) node[below right] {$u^{n}_{i,j}$} -- (other) node[above] {$u^{n+1}_{i,j}$};
    \draw[very thick,->] (left) node[below right] {$u^{n}_{i-1,j}$} -- (other) node[right] {};
    \draw[very thick,->] (back) node[below right] {$u^{n}_{i,j-1}$} -- (other) node[right] {};
    \draw[very thick,->] (right) node[below right] {$u^{n}_{i+1,j}$} -- (other) node[right] {};
    \draw[very thick,->] (front) node[below] {$u^{n}_{i,j+1}$} -- (other) node[right] {};

    \draw[thick, ->] (orig) -- (Horig) node[above] {Time Status};
    \draw[thick, ->] (orig) -- (x) node[right] {dim - $x$};
    \draw[thick, ->] (orig) -- (y) node[above left]  {dim - $y$};;

  \end{tikzpicture}
  \caption{The Schematic Representation the computational spatio-temporal domain of FDTD methods.}
  \label{FIG:Scheme_FDTD_2D_Heat}
\end{figure}



\subsection{Scalability}
High performance computing which also called 
parallel computing
is solving large scale problems using Clusters, with a number of compute nodes and many CPUs.
Parallel computing is many CPUs, thousands of CPUs in most case, are simultaneously processing the data and 
producing the exceptional results, which significantly reduce the time consumption totally.
In this scenario, scaling test is a widely used to investigate the ability of hardware and programs to 
deliver more computational power when the amount of compute resources increase.

\subsubsection{Amdalh's Law and Strong Scaling}
The Amdalh's law for speedup is 
\begin{theorem}\label{THEO:Amdalh's law}
  Let $f_s$ and $f_p$ be the fraction of time spend on purely sequential tasks and that can be parallelized, then 
  the Amdalh's formula
  \cite{AmdalhsLaw} of final speedup is
  \begin{equation}\label{EQ:Amdalh's law}
    S_N = \frac{1}{f_s + \frac{f_p}{N}}
  \end{equation}
\end{theorem}
and it states that for a fixed problem, the upper limits of speedup is restricted by the fraction of sequential parts, 
where the limits is $\lim_{N\to +\infty} S_N = 1 / f_s$, which is called the strong scaling.


\subsubsection{Gustafsson-Barsis' Law}
The Gaustafsson-Barsis' law for speedup is 
\begin{theorem}\label{THEO:GaustafssonLaw}
  Let $f_s$ and $f_p$ be the fraction of time spend on purely sequential tasks and that can be parallelized, then 
  the Gaustafsson-Barsis' formula 
                                                        \cite{GaustafssonLaw}
  of the final speedup is 
  \begin{equation}\label{EQ:GaustafssonLaw}
    S_N = f_s + Nf_p
  \end{equation}
\end{theorem}
With Gaustafsson-Barsis's law, the scaled speedup increases linearly with the number of processes, and there is no upper 
limits for the scaled speedup, which is called weak scaling.

The weak scaling tests of an high performance program runing on cluster is necessary for researching the inner relationships 
between the number of CPUs and the scale of problems.
In order to get better weak scaling performance, the fraction of synchronization among processes and the cost of communications 
plays a minor role when the number of resources increase. 
In the meanwhile, unlike the strong scaling test, the iterations takes for converging on different problem sizes are various as well.
Thus, the weak scaling is more effective if it only consider the scaling for single iteration, where the speedup 
on $N$ CPUs has formula follows
\begin{equation}
  S_N = N\frac{T_1C_N}{C_1T_N} 
\end{equation}
where $T_N$ and $C_N$ denote with the time and epochs that the solver takes for converging on $N$ CPUs.
