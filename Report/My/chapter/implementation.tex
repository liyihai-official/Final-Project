\section{Experiments}
\subsection{Experimental Setup}


I verify the proposed PINN methods on generated dataset, and FDTD methods with hybrid and pure MPI strategies
on the server with two Intel (R) Xeon (R) Platinum 9242 CPU nodes (96 cores per node) and $4$ NUMA nodes per node.
While the dataset is using \texttt{std::m19937} STL random device with given seed $42$ \cite{STL:RANDOM_SEED}.
The PINN models I mentioned are trained using train-from-scratch strategy, and maximin number of training times is $1'000'000$ epochs.
In the setting of learning rate and optimizer, I chose to use Adam with constant learning speed $10^{-3}$.
The details of PDEs are determined in previous section \ref{SEC:Specific_Form}.  

\paragraph{Compiling}
Compiling the program is also a critical important processes.
I chose to use the macros for defining the scale of problems in advance, this is because the compiler will 
have more aggressive optimizations if it knows more predefined parameters.

\paragraph{Running}
For finely manipulate the resource allocation on cluster, following command line \ref{LST:mpirun} is used for this 
the script arguments \texttt{rsc} stands for the resource type such as \texttt{numa}, \texttt{node} and \texttt{socket},
\texttt{--report-bindings} is a error message, for showing the details of threads and CPUs tasks allocated on cluster.
\begin{lstlisting}[
  style=customCpp,
  caption={main command line for launching program on cluster},
  label={LST:mpirun}]
    mpirun --map-by ppr:$ppr:$rsc:pe=$threads --report-bindings <executable> <arguments>
\end{lstlisting}
and \texttt{threads} means the number of threads per resource. The \texttt{ppr} is the number of CPU tasks per resource.
The last \texttt{<argument>} is command line arguments for the program, which is designed by programmer. 
In this case, I designed three type of arguments
\begin{enumerate}
  \item \texttt{-S, -s} Strategy, for specifying the pure mpi, hybrid 0 or 1 strategy.
  \item \texttt{-F, -f} file name, if this argument is defiend, the results will be stored in the file.
  \item \texttt{-H, -h} Helper message, the usage information.
  \item \texttt{-V, -v} Showing the version of program.
\end{enumerate}



\subsubsection{Computational Topology}
The computational topology is critically important when we are programming parallel PDEs solver softwares.
Put the strongly speed-dependent data into the slow memory could make entire program slower.

\paragraph{Cluster}
The cluster we are using for this project is \texttt{Callan} \cite{Callan_TCD} which has 
$2$ CPUs per compute node, and each CPU has $32$ cores with single thread. 
The Non-Uniform Memory Access (NUMA) nodes are layout as following 
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[scale=1, transform shape]
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.5\textwidth]{figure/FIG_Topology_9242.pdf}};
    % \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.58\textwidth]{figure/FIG_Topology_9242.pdf}};     /// overleaf
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        % \draw[help lines, step=1em] (-1em,-1em) grid (30em,20em);    
        % % Draw axes
        % \draw[dashed,->] (-1em,0) -- (30em,0) node[right] {x};
        % \draw[dashed,->] (0,-1em) -- (0,20em) node[above] {y};

      \node at (12em,18em) {$2$ $Scokets$};

      \node at (4.5em,1em) {$DDR4$};
      \node at (19.5em,1em) {$DDR4$};

      \node at (4.5em, 15em) {$DDR4$};
      \node at (19.5em,15em) {$DDR4$};

      \node at (12em,13em) {$Hyper-threads$};
      \node at (12em,5.5em) {$Hyper-threads$};

      \node at (12em,11em) {$48$ $CPUS$};
      \node at (12em,3.5em) {$48$ $CPUS$};
    \end{scope}
  \end{tikzpicture}
  \caption{NUMA topology of single node on Cluster}
  \label{FIG_Topology_Callan}
\end{figure}
Accessing the other NUMA node's memory reduces the bandwidth and also the latency,
though the bandwidth is commonly high enough, the latency can increase by 
$~30\%$ to $~400\%$ \cite{NUMA_Latency_TCD}. 
This latency becomes dangerous when writing shared memory parallel programs.


\subsection{Comparison on single node}
On single node, the CPUs are connected by high-bandwidth, low-latency internal bus which is faster than connection between nodes.
However, for the $4$ total NUMA nodes per compute node, memory accessing between them has higher latency than cache.
Thus, the first tests set were run on the platform with single node, to evaluate the parallelistic performance of heat equation on 2 and 3 dimension spaces with 
3 strategies.


\subsubsection{Strong Scaling}
Figure \ref{FIG:Benchmark:PURE_MPI} visualizes the comparison of my proposed parallelistic program using pure MPI on two dimension space heat euqation with 
the number of CPUs and various problem scales.
Overall, the more CPUs brings more performance among all scales from $512^2$ to the $32'768^2$ but can not break the speedup limits.
Compared with large scale bigger than $4096^2$, the light problems has less speedup as the number of CPUs increases.
By seeing the trend of speedup ratios drop as the CPU gets more, 
the trend can be readily discovered which is the as the scale of problems gets larger, the latter it will have performance-dropping.
Once the problem size is large enough, ($4096^2$ and larger), the solvers can get the more benefits from more CPUs.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{figure/FIG_Benchmark_pure_mpi.pdf}
  \caption{
    Comparison of speedup ratios of strong scaling tests of pure MPI parallelized program. 
    The investigated problem scales are the power of $2$, exponents ranging from $9$ to $15$.
    The number of CPUs are also set as power of $2$, with additional numbers $24$, $48$ and $96$ matched the topologies of CPU.
  }
  \label{FIG:Benchmark:PURE_MPI}
\end{figure}

On the other hand, I also include some unconventionaly number of CPUs in scaling tests such as $24$, $48$ and $96$ for comparison. and 
the results are also shown in the figure \ref{FIG:Benchmark:PURE_MPI}.
From this figure, it is hard to tell the difference of these where it ought to indicate some information about its NUMA structure.
This is because the MPI communication does not strongly effected by memory structure, while the hybrid does.
Figure \ref{FIG:Benchmark:Hybrid} shows the difference, the hybrid strategy brings lower performance with small scale across all CPUs and 
approximately identical in the large scale cases.
The most visible change in figure \ref{FIG:Benchmark:Hybrid_0} is that the speedup ratio of problem with scale $4096^2$ 
exceded the limits on $4$, $8$ and $16$ CPUs which is $1$, $2$, $4$ threads of each MPI process.
We can also see that when the number of threads is $8$ and $4$ MPI processes, the performances on large scale is better than pure MPI parallelism.

\begin{figure}[htbp]
  \centering
  \subfigure[No overlapping comm./comp.]
  {
    \includegraphics[width=0.47\textwidth]{figure/FIG_Benchmark_hybrid_0.pdf}
    \label{FIG:Benchmark:Hybrid_0}
  }
  \hfill
  \subfigure[With overlapping comm./comp.]
  {
    \includegraphics[width=0.47\textwidth]{figure/FIG_Benchmark_hybrid_1.pdf}
    \label{FIG:Benchmark:Hybrid_1}
  }
  \caption{
    Comparison of speedup ratios of strong scaling tests of mater-only parallelized program with overlapping and no overlapping of computation and communication.  
    The vague background is the results of pure MPI parallelization from figure \ref{FIG:Benchmark:PURE_MPI} and problems sclaes are identical as well.
    The number of threads are set to $1$, $2$, $4$, $8$, $16$, and $24$, 
    tasks per CPU are $1$, $2$ and $4$.
  }
  \label{FIG:Benchmark:Hybrid}
\end{figure}

For the other funneled hybrid parallelization, the figure 
            \ref{FIG:Benchmark:Hybrid_1} 
in the appendix shows the details of results,
this strategy has nearly idential performance of master only with no overlapping on large problem scales.
However, the behaviour of it on small scales has a different pattern.
This indicates that the overlappings of computation and communication are not as good as previous one, which means the overload management is not 
well on these tests.


\paragraph{Superliner Speedup}
Conventionally, the actuall speedup won't excedes the theoratical predictions of Amdalh's law.
However, the scaling of two hybrid programs did exceded the limits but exclusively on the problem scale $4096^2$ and $1$ to $8$ threads of each $4$ MPI processes.
Considering the details of the CPU used for these tests, 
\begin{itemize}
  \item It has $4$ NUMA node per CPU and once of which has $12$ CPUs with $2$ threads.
  \item It has \texttt{32KB} L1 data and L1 instruction cache, \texttt{1024KB} L2 cache and \texttt{36608KB} L3 cache.
\end{itemize}
The data type for this solver is \texttt{Double} which takes $8$ bytes, and $4096^2$ \texttt{Double} numbers takes 128 \texttt{MB} to store.
In the case of $4$ MPI processes, each process own a quater of number which uses 32 \texttt{MB} for handling sub-problems.
On the other hand, the L3 cache is 36608 \texttt{KB} = 35.75 \texttt{MB} which is just bigger than the sub-problem scale 32 \texttt{MB}.
When the problem size gets larger, such as $8096^2$ which takes 128 \texttt{MB} to store the sub-problem. 
In such case, the L3 case is no longer able to hold it, thus part of the numbers will be stored in the DDR4 memories which is lower bandwidth and higher latency than cache.
Moreover, due to the CPU enables hyper-threading, a NUMA node actually holds $12$ CPUs, which makes the 
superlinear speedup disappear when the threads is 16 and 24.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.7\textwidth]{figure/FIG_Benchmark_hybrid_1.pdf}
%   \caption{<caption>}
%   \label{FIG:Benchmark:Hybrid_1}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \subfigure[Grid sizes: 150*45*40/25*33*40.]{
%       \includegraphics[width=0.45\textwidth]{weak.png}
%       \label{fig:weak_scaling_1}    
%   }
%   \hfill
%   \subfigure[Grid sizes: 300*90*80/50*65*80]{
%       \includegraphics[width=0.48\textwidth]{weak_scaling_2_efficiency_comparison.png}    
%       \label{fig:weak_scaling_2}
%   }
%   \caption{
%       Weak scaling tests of Cartesian / Cylinder equation with same problem scale on $1$, $2^2$, $3^3$, $4^3$ processors.
%   }
%   \label{fig:weak_scaling}    
% \end{figure}

\subsubsection{Weak Scaling}
The weak scaling tests of an high performance program runing on cluster is necessary for researching the inner relationships 
between the number of CPUs and the scale of problems.
In order to get better weak scaling performance, the fraction of synchronization among processes and the cost of communications 
plays a minor role when the number of resources increase. 

Table \ref{TAB:Benchmark:Weak_PURE_MPI} lists the comparison of three different parallel strategies.
Overall, both three strategies have good weak scaling results across all problem sizes.
For example, according to the Gaustafsson's Law \ref{THEO:GaustafssonLaw}, the program using
overlapped strategy has sequential fraction 
$$f_s = \frac{49.989 - 64}{1-63} \approx 0.222$$
on the problem scale $4096^2$ with 64 CPUs
and $f_s \approx 0.147$ with 16 CPUs.

\begin{table}
  \caption{Weak Scaling on Single Node of 2D Heat Equation}
  \label{TAB:Benchmark:Weak_PURE_MPI}
  \begin{minipage}{\columnwidth}
    \begin{center}
      % \footnotesize % overleaf
      \begin{tabular}{>{\bfseries}p{3cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1cm}}
        \toprule
        \multirow{2}{*}{Strategy}     & \multirow{2}{*}{\bfseries Size} & \multicolumn{3}{c}{\bfseries  Number of CPUs}   \\
                                      &                                  & \bfseries 4   & \bfseries 16   & \bfseries 64   \\
        \midrule
        Pure MPI      & \multirow{3}{*}{$512^2$}      & 4.006  & 12.497  & 47.849 \\
        No Overlap    &                               & 2.876  & 11.206  & 42.754 \\
        With Overlap  &                               & 3.173  & 10.818  & 42.282 \\
        \midrule
        Pure MPI      & \multirow{3}{*}{$1024^2$}     & 3.838  & 9.304   & 33.707 \\
        No Overlap    &                               & 3.947  & 12.995  & 33.447 \\
        With Overlap  &                               & 4.024  & 12.932  & 33.361 \\
        \midrule
        Pure MPI      & \multirow{3}{*}{$2048^2$}     & 2.376  & 8.245   & 31.203 \\
        No Overlap    &                               & 3.874  & 8.972   & 31.510 \\
        With Overlap  &                               & 3.740  & 8.989   & 31.430 \\
        \midrule
        Pure MPI      & \multirow{3}{*}{$4096^2$}     & 3.543  & 8.245   & 31.203 \\
        No Overlap    &                               & 3.953  & 13.799  & 49.515 \\
        With Overlap  &                               & 3.948  & 13.800  & 49.989 \\
        \bottomrule
      \end{tabular}
    \end{center}
    % \bigskip
    % \footnotesize\emph{Source:} This is source 
  \end{minipage}
\end{table}



\subsection{Comparison on multi-node}
\subsection{Comparison}

% \subsection{Finite Difference Methods}
% \subsubsection{Pure Message Passing Parallel}
% \subsubsection{Hybrid Parallel}


% \subsection{Physics Informed Neural Networks}
% \subsubsection{CUDA parallel}
% \subsubsection{Hybrid Parallel}


\subsection{Visualization}












% Table 
% lists the results 

% \begin{table}
%   \caption{Strong Scaling on Single Node of 2D Heat Equation}
%   \label{}
%   \begin{minipage}{\columnwidth}
%     \begin{center}
%       \begin{tabular}{lcccccc}
%         \toprule
%         Scale & $1024^2$ & $2048^2$ & $4096^2$  & $8192^2$  & $16384^2$ & $32768^2$\\
%         \midrule
%         2     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         4     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         8     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         16     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         32     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         48     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         64     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         96     & 1        & 1        &   1       & 1         & 1         & 1 \\
%         \bottomrule
%       \end{tabular}
%     \end{center}
%     % \bigskip
%     % \footnotesize\emph{Source:} This is source 
%   \end{minipage}
% \end{table}
