% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries



@Article{Abril07,
  author        = "Patricia S. Abril and Robert Plant",
  title         = "The patent holder's dilemma: Buy, sell, or troll?",
  journal       = "Communications of the ACM",
  volume        = "50",
  number        = "1",
  month         = jan,
  year          = "2007",
  pages         = "36--44",
  doi           = "10.1145/1188913.1188915",
  url           = "http://doi.acm.org/10.1145/1219092.1219093",
  note          = "",
}

@article{SolveKorPDE,
  author = {Beck, Christian and Becker, Sebastian and Grohs, Philipp and Jaafari, Nor and Jentzen, Arnulf},
  title = {Solving the Kolmogorov PDE by Means of Deep Learning},
  journal = {Journal of Scientific Computing},
  volume = {88},
  number = {3},
  pages = {73},
  year = {2021},
  doi = {10.1007/s10915-021-01590-0},
  url = {https://doi.org/10.1007/s10915-021-01590-0},
  issn = {1573-7691}
}

@article{DNN-HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@manual{OpenMP,
  title        = {OpenMP Application Programming Interface},
  author       = {OpenMP Architecture Review Board},
  year         = {2023},
  edition      = {5.2},
  url          = {https://www.openmp.org/specifications/},
  organization = {OpenMP Architecture Review Board},
  note         = {Accessed: 2024-09-07}
}


@manual{MPI,
  title        = {MPI: A Message-Passing Interface Standard},
  author       = {Message Passing Interface Forum},
  year         = {2021},
  edition      = {Version 4.0},
  url          = {https://www.mpi-forum.org/docs/},
  organization = {Message Passing Interface Forum},
  note         = {Accessed: 2024-09-07}
}

@article{CNN,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = {may},
pages = {84–90},
numpages = {7}
}


@article{PINN,
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  title = {Physics-informed machine learning},
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  year = {2021},
  month = {June},
  doi = {10.1038/s42254-021-00314-5},
  url = {https://doi.org/10.1038/s42254-021-00314-5},
  issn = {2522-5820},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.}
}

@ARTICLE{GPT,
  author={Yenduri, Gokul and Ramalingam, M. and Selvi, G. Chemmalar and Supriya, Y. and Srivastava, Gautam and Maddikunta, Praveen Kumar Reddy and Raj, G. Deepti and Jhaveri, Rutvij H. and Prabadevi, B. and Wang, Weizheng and Vasilakos, Athanasios V. and Gadekallu, Thippa Reddy},
  journal={IEEE Access}, 
  title={GPT (Generative Pre-Trained Transformer)— A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions}, 
  year={2024},
  volume={12},
  number={},
  pages={54608-54649},
  keywords={Natural language processing;Solid modeling;Artificial intelligence;Surveys;Task analysis;Reviews;Transformers;Generative pre-trained transformer;natural language processing;artificial intelligence},
  doi={10.1109/ACCESS.2024.3389497}}



@ARTICLE{AmdalhsLaw,
  author={Amdahl, Gene M.},
  journal={IEEE Solid-State Circuits Society Newsletter}, 
  title={Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities, Reprinted from the AFIPS Conference Proceedings, Vol. 30 (Atlantic City, N.J., Apr. 18–20), AFIPS Press, Reston, Va., 1967, pp. 483–485, when Dr. Amdahl was at International Business Machines Corporation, Sunnyvale, California}, 
  year={2007},
  volume={12},
  number={3},
  pages={19-20},
  keywords={Computers;Hardware;Data mining;Degradation;Memory management;Organizations;Performance evaluation},
  doi={10.1109/N-SSC.2007.4785615}}

@article{GaustafssonLaw,
    author = {Gustafson, John L.},
    title = {Reevaluating Amdahl's law},
    year = {1988},
    issue_date = {May 1988},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {31},
    number = {5},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/42411.42415},
    doi = {10.1145/42411.42415},
    journal = {Commun. ACM},
    month = {may},
    pages = {532–533},
    numpages = {2}
}

@Inbook{Moore_Law,
author="Gustafson, John L.",
editor="Padua, David",
title="Moore's Law",
bookTitle="Encyclopedia of Parallel Computing",
year="2011",
publisher="Springer US",
address="Boston, MA",
pages="1177--1184",
isbn="978-0-387-09766-4",
doi="10.1007/978-0-387-09766-4_81",
url="https://doi.org/10.1007/978-0-387-09766-4_81"
}

@misc{Matrix_Calculus,
  author       = {{MIT OpenCourseWare}},
  title        = {Matrix Calculus for Machine Learning and Beyond},
  howpublished = {\url{https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/}},
  year         = {2023},
  note         = {Accessed: 2024-09-07}
}


@article{AD,
author = {Baydin, At\i{}l\i{}m G\"{u}nes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
title = {Automatic differentiation in machine learning: a survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {5595–5637},
numpages = {43},
keywords = {backpropagation, differentiable programming}
}



@article{CFL_limitation,
title = {Enforcing the Courant–Friedrichs–Lewy condition in explicitly conservative local time stepping schemes},
journal = {Journal of Computational Physics},
volume = {359},
pages = {93-105},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118300184},
author = {Nickolay Y. Gnedin and Vadim A. Semenov and Andrey V. Kravtsov},
keywords = {Numerical methods, Computational fluid dynamics, Partial differential equations},
abstract = {An optimally efficient explicit numerical scheme for solving fluid dynamics equations, or any other parabolic or hyperbolic system of partial differential equations, should allow local regions to advance in time with their own, locally constrained time steps. However, such a scheme can result in violation of the Courant–Friedrichs–Lewy (CFL) condition, which is manifestly non-local. Although the violations can be considered to be “weak” in a certain sense and the corresponding numerical solution may be stable, such calculation does not guarantee the correct propagation speed for arbitrary waves. We use an experimental fluid dynamics code that allows cubic “patches” of grid cells to step with independent, locally constrained time steps to demonstrate how the CFL condition can be enforced by imposing a constraint on the time steps of neighboring patches. We perform several numerical tests that illustrate errors introduced in the numerical solutions by weak CFL condition violations and show how strict enforcement of the CFL condition eliminates these errors. In all our tests the strict enforcement of the CFL condition does not impose a significant performance penalty.}
}


@manual{BOOST_MPI,
  title        = {Boost.MPI: C++ library for message passing},
  author       = {Boost C++ Libraries},
  year         = {2024},
  note         = {Version 1.82.0},
  url          = {https://www.boost.org/doc/libs/release/doc/html/mpi.html},
  organization = {Boost}
}



@article{Transformer_PINN,
  author    = {Goswami, Somdatta and Kontolati, Katiana and Shields, Michael D. and Karniadakis, George Em},
  title     = {Deep transfer operator learning for partial differential equations under conditional shift},
  journal   = {Nature Machine Intelligence},
  volume    = {4},
  number    = {12},
  pages     = {1155--1164},
  year      = {2022},
  month     = {December},
  doi       = {10.1038/s42256-022-00569-2},
  url       = {https://doi.org/10.1038/s42256-022-00569-2},
  issn      = {2522-5839},
  abstract  = {Transfer learning enables the transfer of knowledge gained while learning to perform one task (source) to a related but different task (target), hence addressing the expense of data acquisition and labelling, potential computational power limitations and dataset distribution mismatches. We propose a new transfer learning framework for task-specific learning (functional regression in partial differential equations) under conditional shift based on the deep operator network (DeepONet). Task-specific operator learning is accomplished by fine-tuning task-specific layers of the target DeepONet using a hybrid loss function that allows for the matching of individual target samples while also preserving the global properties of the conditional distribution of the target data. Inspired by conditional embedding operator theory, we minimize the statistical distance between labelled target data and the surrogate prediction on unlabelled target data by embedding conditional distributions onto a reproducing kernel Hilbert space. We demonstrate the advantages of our approach for various transfer learning scenarios involving nonlinear partial differential equations under diverse conditions due to shifts in the geometric domain and model dynamics. Our transfer learning framework enables fast and efficient learning of heterogeneous tasks despite considerable differences between the source and target domains.}
}



@manual{NVIDIA_HB200_PAPER,
  title        = {NVIDIA H200 Tensor Core GPU},
  author       = {NVIDIA Corporation},
  year         = {2024},
  note         = {HBM2 High Bandwidth Memory Technology},
  url          = {https://www.nvidia.com/en-us/data-center/h200/},
  organization = {NVIDIA},
}


@book{GermundNMSCV1P122,
author = {Dahlquist, Germund and Bjrck, ke},
title = {Numerical Methods in Scientific Computing: Volume 1},
year = {2008},
isbn = {0898716446},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {This new book from the authors of the classic book Numerical Methods addresses the increasingly important role of numerical methods in science and engineering. More cohesive and comprehensive than any other modern textbook in the field, it combines traditional and well-developed topics with other material that is rarely found in numerical analysis texts, such as interval arithmetic, elementary functions, operator series, convergence acceleration, and continued fractions. Although this volume is self-contained, more comprehensive treatments of matrix computations will be given in a forthcoming volume. A supplementary Website contains three appendices: an introduction to matrix computations; a description of Mulprec, a MATLAB multiple precision package; and a guide to literature, algorithms, and software in numerical analysis. Review questions, problems, and computer exercises are also included. For use in an introductory graduate course in numerical analysis and for researchers who use numerical methods in science and engineering.}
}

@techreport{IEEE_754,
  author       = {William Kahan},
  title        = {IEEE 754: An Interview Saga},
  year         = {1998},
  institution  = {University of California, Berkeley},
  url          = {https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF},
  note         = {Accessed: 2024-09-07}
}

@misc{LibTorch,
  title        = {LibTorch: PyTorch C++ API},
  author       = {PyTorch Team},
  year         = {2024},
  howpublished = {\url{https://pytorch.org/cppdocs/}},
  note         = {Accessed: 2024-09-07},
  organization = {Meta AI},
}


@misc{openmp-hybrid-mpi-openmp,
  author       = {OpenMP},
  title        = {SC13 Tutorial: Hybrid MPI/OpenMP Parallel Programming},
  year         = {2013},
  howpublished = {\url{https://www.openmp.org/events/sc13-tutorial-hybrid-mpi-openmp-parallel-programming/}},
  note         = {Accessed: 2024-09-07}
}


@misc{STL:RANDOM_SEED,
  author       = {cppreference.com},
  title        = {C++ documentation: Random number generation},
  year         = {2024},
  howpublished = {\url{https://en.cppreference.com/w/cpp/numeric/random}},
  note         = {Accessed: 2024-09-07}
}



@misc{NUMA_Latency_TCD,
  author       = {MAP55621-HPC Hardware and Architectures},
  title        = {Trinity College Dublin Lecture 14},
  year         = {2024},
  howpublished = {\url{HPCHardware_14.pdf}},
  note         = {Accessed: 2024-09-07}
}



@book{Monte_Carlo_Method,
  author    = {Nicholas Metropolis and Stanislaw Ulam},
  title     = {The Monte Carlo Method},
  year      = {1949},
  publisher = {Journal of the American Statistical Association},
  volume    = {44},
  number    = {247},
  pages     = {335--341}
}


@misc{intel-xeon-9242,
  author       = {Intel Corporation},
  title        = {Intel\textregistered Xeon\textregistered Platinum 9242 Processor (71.5M Cache, 2.30 GHz)},
  year         = {2024},
  howpublished = {\url{https://ark.intel.com/content/www/us/en/ark/products/194145/intel-xeon-platinum-9242-processor-71-5m-cache-2-30-ghz.html}},
  note         = {Accessed: 2024-09-07}
}

@misc{rtx4090,
  author       = {NVIDIA Corporation},
  title        = {NVIDIA GeForce RTX 4090 Graphics Card},
  year         = {2024},
  howpublished = {\url{https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/}},
  note         = {Accessed: 2024-09-07}
}

@misc{gperftools,
  author       = {gperftools Development Team},
  title        = {gperftools: CPU Profiling with \texttt{CPUPROFILE}},
  year         = {2024},
  howpublished = {\url{https://gperftools.github.io/gperftools/cpuprofile.html}},
  note         = {Accessed: 2024-09-07}
}